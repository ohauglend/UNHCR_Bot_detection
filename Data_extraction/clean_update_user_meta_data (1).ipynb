{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The notebook is to be distributed to the remote servers for data collection, it will run on a local notebook and store the updated meta data on that server"
      ],
      "metadata": {
        "id": "m7rjG2eEafbI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ixsoZc4aQdb",
        "outputId": "866e964c-0826-4e45-8133-d5966d2f5e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting GeoText\n",
            "  Downloading geotext-0.4.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 7.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: GeoText\n",
            "Successfully installed GeoText-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2022.11.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 17.9 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.9.24)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16369 sha256=29002b8d79a358e3c13d8eb64d8c69ef8ce0729f33ebf799c43e08286b33675c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gender-guesser\n",
            "  Downloading gender_guesser-0.4.0-py2.py3-none-any.whl (379 kB)\n",
            "\u001b[K     |████████████████████████████████| 379 kB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: gender-guesser\n",
            "Successfully installed gender-guesser-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==4.8.0\n",
            "  Downloading tweepy-4.8.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.8.0) (1.3.1)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy==4.8.0) (3.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2.1.1)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed requests-2.28.1 tweepy-4.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=d1990a20a42884ef70da2774085c044eb464cd292d3b4502c77a01088efe91ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#Installing required library for getting location:\n",
        "!pip install GeoText\n",
        "from geotext import GeoText\n",
        "\n",
        "#Installing required library for language detection:\n",
        "#Need >= 3.0.0\n",
        "!pip install googletrans==3.1.0a0\n",
        "from googletrans import Translator\n",
        "\n",
        "#Installing the required libraries for extracting twitter data\n",
        "!pip install gender-guesser\n",
        "#Need >= 4.8.0\n",
        "!pip install tweepy==4.8.0\n",
        "\n",
        "#Setting up email notification for when errors are triggered\n",
        "import smtplib \n",
        "import socket\n",
        "\n",
        "import tweepy as tw\n",
        "from tweepy.errors import TooManyRequests\n",
        "from tweepy.errors import NotFound\n",
        "from tweepy.errors import Forbidden\n",
        "#NotFound: 404 Not Found\n",
        "\n",
        "#from datetime import datetime, timedelta\n",
        "import gender_guesser.detector\n",
        "import copy\n",
        "from datetime import date, timedelta, datetime\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "#Library needed for language detection\n",
        "! pip install langdetect\n",
        "from langdetect import detect, detect_langs, LangDetectException"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the updating user meta data - need to change in VM:\n",
        "1. default keys in update_df_user_meta\n",
        "2.  input_df_path and output_folder_path in updating_and_storing_user_meta_data"
      ],
      "metadata": {
        "id": "nSXcQwqzroUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Keys used in the different VMs - #.XX is used to reference which VM is used\n",
        "\n",
        "#.84\n",
        "def update_df_user_meta(input_df,\n",
        "                        input_consumer_key= CONSUMER_KEY,\n",
        "                        input_consumer_secret= CONSUMER_KEY_SECRET,\n",
        "                        input_access_token= ACCESS_TOKEN,\n",
        "                        input_access_token_secret= ACCESS_TOKEN_SECRET,\n",
        "                        input_bearer_token = BEARER_TOKEN\n",
        "                        ):\n",
        "\n",
        "#.94\n",
        "def update_df_user_meta(input_df,\n",
        "                        input_consumer_key= CONSUMER_KEY,\n",
        "                        input_consumer_secret= CONSUMER_KEY_SECRET,\n",
        "                        input_access_token= ACCESS_TOKEN,\n",
        "                        input_access_token_secret= ACCESS_TOKEN_SECRET,\n",
        "                        input_bearer_token = BEARER_TOKEN\n",
        "                        ):\n",
        "\n",
        "#.95\n",
        "def update_df_user_meta(input_df,\n",
        "                        input_consumer_key= CONSUMER_KEY,\n",
        "                        input_consumer_secret= CONSUMER_KEY_SECRET,\n",
        "                        input_access_token= ACCESS_TOKEN,\n",
        "                        input_access_token_secret= ACCESS_TOKEN_SECRET,\n",
        "                        input_bearer_token = BEARER_TOKEN\n",
        "                        ):\n",
        "\n",
        "\n",
        "The function takes in a subsetted df, queries the meta data of the users and populates the subsetted df.\n",
        "\n",
        "It takes in a df as its only non default argument. \n",
        "The Twitter API credentials need to be set in the default arguments of the function\n",
        "\n",
        "The function returns the updated df, a dictionary of user ids that failed some sort of validation criteria, and the user id of the last user to be populated.\n",
        "\n",
        "\n",
        "\"\"\"  \n",
        "def update_df_user_meta(input_df,\n",
        "                        input_consumer_key= CONSUMER_KEY,\n",
        "                        input_consumer_secret= CONSUMER_KEY_SECRET,\n",
        "                        input_access_token= ACCESS_TOKEN,\n",
        "                        input_access_token_secret= ACCESS_TOKEN_SECRET,\n",
        "                        input_bearer_token = BEARER_TOKEN\n",
        "                        ):\n",
        "\n",
        "  ##Connecting to the api, setting the keys\n",
        "  auth = tw.OAuthHandler(input_consumer_key, input_consumer_secret)\n",
        "  if(input_access_token_secret!= \"\"):\n",
        "    auth.set_access_token(input_access_token, input_access_token_secret)\n",
        "\n",
        "  api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "  #Creating a copy of the inputted df\n",
        "  df = copy.deepcopy(input_df)\n",
        "\n",
        "\n",
        "  #Creating a dictionary used for keeping track of faulty user_ids\n",
        "  bad_user_ids = {\"get_user_from_api\":[],\"user_created_at_match\":[],\"user_id_match\":[], \"user_name_matc\":[], \"user_screen_name_match\":[]}\n",
        "\n",
        "  #Looping through the df\n",
        "  for i in tqdm(range(len(df))):\n",
        "    \n",
        "    #Ran into some issues with to short time inntervals between queries\n",
        "    time.sleep(1)\n",
        "    \n",
        "    try:\n",
        "      #ilocing the user_id sometimes returns a format that is non compatible with api requirements -> calling int() \n",
        "      user = api.get_user(user_id = int(df.user_id.iloc[i]))\n",
        "      \n",
        "    #Error handling if the api cannot retrieve the user from the API - either because it is private or blocked\n",
        "    except NotFound:\n",
        "      bad_user_ids[\"get_user_from_api\"].append(df.user_id.iloc[i])\n",
        "      warnings.warn(\"Twitter API could not retrive info for user: \"+ str(df.user_id.iloc[i]), DeprecationWarning)\n",
        "      continue\n",
        "    \n",
        "    #Error habndling in case the user is suspended\n",
        "    except Forbidden as forbidden_error:\n",
        "      if \"User has been suspended\" in repr(forbidden_error):\n",
        "        continue\n",
        "      print(\"except triggered, last user_id = \" +str(df.user_id.iloc[i-1])+\"\\nForbidden exception triggered: \"+str(forbidden_error))\n",
        "      return df, bad_user_ids, df.user_id.iloc[i-1]\n",
        "\n",
        "    #Error handling in case there is something faulty with the run -> the script will return the current state of df, bad_user_id, last_index_to_be_populated\n",
        "    except Exception as e:\n",
        "      print(\"except triggered, last user_id = \" +str(df.user_id.iloc[i-1])+\"\\nException triggered: \"+str(e))\n",
        "      return df, bad_user_ids, df.user_id.iloc[i-1]\n",
        "\n",
        "    \n",
        "    #Validating user data that should not change when comparing the API input to the dataset:\n",
        "      #Need to convert df date back from unix timestamp and compare on date, due to rounding\n",
        "      #If found, the system stores the user id in bad_user_ids and skips the iteration of the df loop\n",
        "    if datetime.fromtimestamp(df.created_at_1.iloc[i]/1000).date() != user.created_at.date():\n",
        "      warnings.warn(\"User retrived from Twitter API has a different value for created_at than in the data, this is true for: \"+ str(df.user_id.iloc[i]), DeprecationWarning)\n",
        "      bad_user_ids[\"user_created_at_match\"].append(df.user_id.iloc[i])\n",
        "      continue\n",
        "\n",
        "    #Same for user_id -> this is a non-mutable field\n",
        "    if df.user_id.iloc[i] != user.id:\n",
        "      warnings.warn(\"User retrived from Twitter API has a different value for user_id than in the data, this is true for: \"+ str(df.user_id.iloc[i]), DeprecationWarning)\n",
        "      bad_user_ids[\"user_id_match\"].append(df.user_id.iloc[i])\n",
        "      continue\n",
        "\n",
        "    \n",
        "    #Allowing the user to change name and screen_name as this is possible:\n",
        "    #https://help.twitter.com/en/managing-your-account/change-twitter-handle\n",
        "    if df.user_name.iloc[i] != user.name:\n",
        "      bad_user_ids[\"user_name_matc\"].append(df.user_id.iloc[i])\n",
        "      df.at[i, 'user_name'] = user.name\n",
        "\n",
        "\n",
        "    if df.user_screen_name.iloc[i] != user.screen_name:\n",
        "      bad_user_ids[\"user_screen_name_match\"].append(df.user_id.iloc[i])\n",
        "      df.at[i, 'user_screen_name'] = user.screen_name\n",
        "\n",
        "\n",
        "    #user_profile_location - if the user has not specified profile location, user.profile_location.get('name') will return an AttributeError   \n",
        "    #Since we have performed GeoText on the location in pre-processing the location is in format Country, city, city, city, ....\n",
        "    #This is the same as what is returned from Twitter API when calling user.profile_location.get('name') -> we choose the location with the most ,\n",
        "    if type(user.profile_location) == dict:\n",
        "        df.at[i,'user_location'] = user.profile_location.get('name')\n",
        "\n",
        "\n",
        "    #user_description - update if new\n",
        "    if type(user.description) == str:\n",
        "      df.at[i, 'user_description'] = user.description\n",
        "\n",
        "\n",
        "    #user_url - update if new\n",
        "    if type(user.url) == str:\n",
        "        df.at[i,'user_url'] = user.url\n",
        "\n",
        "    \n",
        "    #protected - update if new\n",
        "    if type(user.protected) == bool:\n",
        "        df.at[i,'protected'] = float(user.protected)\n",
        "\n",
        "\n",
        "    #followers_count - update if new\n",
        "    if type(user.followers_count) == int:\n",
        "      df.at[i,'followers_count'] = float(user.followers_count)\n",
        "\n",
        "    \n",
        "    #friends_count - update if new\n",
        "    if type(user.friends_count) == int:\n",
        "      df.at[i,'friends_count'] = float(user.friends_count)\n",
        "\n",
        "\n",
        "    #listed_count - update if new\n",
        "    if type(user.listed_count) == int:\n",
        "      df.at[i,'listed_count'] = float(user.listed_count)\n",
        "\n",
        "\n",
        "\n",
        "    #favourites_count - update if new\n",
        "    if type(user.favourites_count) == int:\n",
        "      df.at[i,'favourites_count'] = float(user.favourites_count)\n",
        "\n",
        "\n",
        "    #utc_offset - update if new\n",
        "    #missing values are marked as np.nan meaning comparing on type (float) does not work\n",
        "    if type(user.utc_offset) == str:\n",
        "      df.at[i,'utc_offset'] = user.utc_offset\n",
        "\n",
        "\n",
        "    #time_zone - update if new\n",
        "    if type(user.time_zone) == str:\n",
        "      df.at[i,'time_zone'] = user.time_zone\n",
        "\n",
        "\n",
        "    #geo_enabled - update if new\n",
        "    if type(user.geo_enabled) == bool:\n",
        "      df.at[i,'geo_enabled'] = float(user.geo_enabled)\n",
        "\n",
        "\n",
        "    #verified - might change - for now: all acounts except 3 in dataset have binary labels if verified or not - update if new:\n",
        "    if type(user.verified) == bool:\n",
        "      df.at[i,'verified'] = float(user.verified)\n",
        "\n",
        "      \n",
        "    #statuses_count -update if new\n",
        "    if type(user.statuses_count) == int:\n",
        "      df.at[i,'statuses_count'] = float(user.statuses_count)\n",
        "\n",
        "\n",
        "    #lang - update if new\n",
        "    if type(user.lang) == str:\n",
        "      df.at[i,'lang'] = user.lang\n",
        "\n",
        "\n",
        "    #contributors_enabled - update if new\n",
        "    if type(user.contributors_enabled) == bool:\n",
        "      df.at[i,'contributors_enabled'] = float(user.contributors_enabled)\n",
        "\n",
        "\n",
        "    #is_translator - update if new\n",
        "    if type(user.is_translator) == bool:\n",
        "      df.at[i,'is_translator'] = float(user.is_translator)\n",
        "\n",
        "\n",
        "    #is_translation_enabled - update if new\n",
        "    if type(user.is_translation_enabled) == bool:\n",
        "      df.at[i,'is_translation_enabled'] = float(user.is_translation_enabled)\n",
        "\n",
        "\n",
        "    #profile_background_color - update if new\n",
        "    if type(user.profile_background_color) == str:\n",
        "      df.at[i,'profile_background_color'] = user.profile_background_color\n",
        "\n",
        "\n",
        "    #profile_background_image_url - update if new\n",
        "    if type(user.profile_background_image_url) == str:\n",
        "      df.at[i,'profile_background_image_url'] = user.profile_background_image_url\n",
        "\n",
        "\n",
        "    #profile_background_image_url_https - update if new\n",
        "    if type(user.profile_background_image_url_https) == str:\n",
        "      df.at[i,'profile_background_image_url_https'] = user.profile_background_image_url_https\n",
        "\n",
        "\n",
        "    #profile_background_tile - update if new\n",
        "    if type(user.profile_background_tile) == bool:\n",
        "      df.at[i,'profile_background_tile'] = float(user.profile_background_tile)\n",
        "\n",
        "\n",
        "    #profile_image_url - update if new\n",
        "    if type(user.profile_image_url) == str:\n",
        "      df.at[i,'profile_image_url'] = user.profile_image_url\n",
        "      \n",
        "\n",
        "    #profile_image_url_https - update if new\n",
        "    if type(user.profile_image_url_https) == str:\n",
        "      df.at[i,'profile_image_url_https'] = user.profile_image_url_https\n",
        "\n",
        "\n",
        "    #profile_link_color - update if new\n",
        "    if type(user.profile_link_color) == str:\n",
        "      df.at[i,'profile_link_color'] = user.profile_link_color\n",
        "\n",
        "\n",
        "    #profile_sidebar_border_color - update if new\n",
        "    if type(user.profile_sidebar_border_color) == str:\n",
        "      df.at[i,'profile_sidebar_border_color'] = user.profile_sidebar_border_color\n",
        "\n",
        "\n",
        "    #profile_sidebar_fill_color - update if new\n",
        "    if type(user.profile_sidebar_fill_color) == str:\n",
        "      df.at[i,'profile_sidebar_fill_color'] = user.profile_sidebar_fill_color\n",
        "\n",
        "\n",
        "    #profile_text_color - update if new\n",
        "    if type(user.profile_text_color) == str:\n",
        "      df.at[i,'profile_text_color'] = user.profile_text_color\n",
        "      \n",
        "\n",
        "    #profile_use_background_image - update if new\n",
        "    if type(user.profile_use_background_image) == bool:\n",
        "      df.at[i,'profile_use_background_image'] = float(user.profile_use_background_image)\n",
        "\n",
        "\n",
        "    #translator_type - update if new\n",
        "    if type(user.translator_type) == str:\n",
        "      df.at[i,'translator_type'] = user.translator_type\n",
        "\n",
        "\n",
        "    #withheld - update if new\n",
        "    if type(user.withheld_in_countries) == list:\n",
        "      df.at[i,'withheld'] = user.withheld_in_countries\n",
        "\n",
        "\n",
        "    #has_extended_profile - update if new\n",
        "    if type(user.has_extended_profile) == bool:\n",
        "      df.at[i,'has_extended_profile'] = float(user.has_extended_profile)\n",
        "\n",
        "\n",
        "    #default_profile - update if new\n",
        "    if type(user.default_profile) == bool:\n",
        "      df.at[i,'default_profile'] = float(user.default_profile)\n",
        "\n",
        "\n",
        "    #default_profile_image - update if new\n",
        "    if type(user.default_profile_image) == bool:\n",
        "      df.at[i,'default_profile_image'] = float(user.default_profile_image)\n",
        "\n",
        "\n",
        "      \n",
        "  return df, bad_user_ids, df.iloc[-1].user_id\n"
      ],
      "metadata": {
        "id": "6TyvCEgmawwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def updating_and_storing_user_meta_data(input_df_path,\n",
        "                                        output_folder_path,\n",
        "                                        increment_intervall = 300\n",
        "                                        ):\n",
        "  \n",
        "  \"\"\"\n",
        "  The function takes in an input file path and a othput folder path as its only non-default arguments.\n",
        "    output_folder_path is to be specified without the final /\n",
        "\n",
        "  The increment interval controls how often the function stores updated data \n",
        "  and at which intervals it subsets the df and sends it to update_df_user_meta.\n",
        "  It is defualted to 300 as this is the rate limit of the elevated API\n",
        "\n",
        "  The frequent storage is needed as the function is intended to be used without supervision on a virtual machine.\n",
        "\n",
        "  The function has no return\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  #Reading in the df from input_df_path\n",
        "  df = pd.read_json(input_df_path)\n",
        "\n",
        "  #Reseting index as the main df has been split when creating the distributed datasets\n",
        "  df = df.reset_index(drop=True)\n",
        "  \n",
        "  #Instantiating dummy variables updated with returns from update_df_user_meta\n",
        "    #Creating an empty df to be populated incrementally\n",
        "  df2 = copy.deepcopy(df)\n",
        "  df2 = df2.dropna()\n",
        "  bad_user_ids = {\"get_user_from_api\":[],\"user_created_at_match\":[],\"user_id_match\":[], \"user_name_matc\":[], \"user_screen_name_match\":[]}\n",
        "  index_of_last_user_to_be_populated = -1\n",
        "\n",
        "\n",
        "  #Looping through df from input_df_path\n",
        "  for i in range(0,len(df),increment_intervall):\n",
        "   \n",
        "    try:\n",
        "      \n",
        "      #Subsetting df and feeding into update_df_user_meta\n",
        "      d = df.iloc[index_of_last_user_to_be_populated+1:i+increment_intervall].reset_index(drop=True)\n",
        "      df3, bad_user_ids2, last_user_to_be_populated = update_df_user_meta(d)\n",
        "      \n",
        "      #Extending df2 with this iterations return value\n",
        "      df2 = df2.append(df3)\n",
        "      df2 = df2.reset_index(drop=True)\n",
        "\n",
        "      #extending the bad_user_ids dictionary \n",
        "      [bad_user_ids.setdefault(k, []).extend(v) for k,v in bad_user_ids2.items()]\n",
        "\n",
        "      #Updating last index to be populated\n",
        "      index_of_last_user_to_be_populated = df.index[df.user_id == last_user_to_be_populated][0]\n",
        "\n",
        "      #Storing relevant data:\n",
        "      df2.to_json(output_folder_path+'/start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'.json')\n",
        "        \n",
        "      #Creating a df from bad_user_ids with None to make sure all values are the same length\n",
        "      pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in bad_user_ids.items() ])).to_json(output_folder_path+'/bad_user_ids_start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'.json')\n",
        "\n",
        "      print('\\n\\nSuccessfully stored start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'\\n\\n')\n",
        "    \n",
        "    #Error hanlding - sending an email to recipricant from sender\n",
        "    except Exception as e:\n",
        "      \n",
        "      #extending the get_user_from_api from this iteration\n",
        "      bad_user_ids['get_user_from_api'].extend(df.user_id.iloc[index_of_last_user_to_be_populated+1:i+increment_intervall].tolist())\n",
        "      \n",
        "      #Storing bad_user_ids\n",
        "      pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in bad_user_ids.items() ])).to_json(output_folder_path+'/bad_user_ids_start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(last_user_to_be_populated)+'.json')\n",
        "     \n",
        "      print(\"Main for loop failed, Error: \"+ str(e) + \"i: \"+str(i) + \"increment_intervall: \" + str(increment_intervall))\n",
        "      \n",
        "      #Sending RECIPRICANT@cbs.dk an email if exception is triggered\n",
        "      #Getting local IP\n",
        "      s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "      s.connect((\"8.8.8.8\", 80))\n",
        "      LOCAL_IP = s.getsockname()[0]\n",
        "      s.close()\n",
        "\n",
        "      #Setting up connection to sender email - setup is used for gmail sender - RECIPRICANT can be any mail\n",
        "      with smtplib.SMTP_SSL('smtp.gmail.com', 465) as connection:  \n",
        "        email_address = 'SENDER@gmail.com'\n",
        "        email_password = 'TWO_FACTOR_AUTHENTICATION_PASSWORD'\n",
        "        connection.login(email_address, email_password )\n",
        "        connection.sendmail(from_addr=email_address, to_addrs='RECIPRICANT@cbs.dk', \n",
        "        msg=\"IP: \"+str(LOCAL_IP)+\"\\nMain for loop failed, Error: \"+ str(e) + \"i: \"+str(i) + \"increment_intervall: \" + str(increment_intervall)\n",
        "        )\n",
        "\n",
        "\n",
        "  #Storing one final copy after exiting the for loop\n",
        "  df2.to_json(output_folder_path+'/FINAL_start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'.json')\n",
        "  \n",
        "  #Creating a df from bad_user_ids with None to make sure all values are the same length\n",
        "  pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in bad_user_ids.items() ])).to_json(output_folder_path+'/FINAL_bad_user_ids_start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'.json')\n",
        "\n",
        "\n",
        "  print(\"File stored at: \"+ str(output_folder_path+'/FINAL_start_user_id_'+str(df.user_id.iloc[0])+'_end_user_id'+str(df2.user_id.iloc[-1])+'.json'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J825VfKerwxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updating_and_storing_user_meta_data(input_df_path = 'input_df_path',\n",
        "                                    output_folder_path = 'output_folder_path'\n",
        "                                    increment_intervall = 300\n",
        "                                        )"
      ],
      "metadata": {
        "id": "p_zPgKhPI9-e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}