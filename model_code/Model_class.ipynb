{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "24gHZhQz1r-d",
        "TGH-eZKU1r-d"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import and model code"
      ],
      "metadata": {
        "id": "UbiSxNf5kOvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install mlflow\n",
        "import mlflow\n",
        "import mlflow.keras\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "from datetime import date, timedelta, datetime\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import math\n",
        "\n",
        "!pip install tensorflow_decision_forests==1.2.0\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "#Need >= 4.8.0\n",
        "!pip install tweepy==4.8.0\n",
        "import smtplib \n",
        "import socket\n",
        "import tweepy as tw\n",
        "from tweepy.errors import TooManyRequests\n",
        "from tweepy.errors import Forbidden, NotFound, TwitterServerError\n",
        "from tweepy.errors import BadRequest\n",
        "import copy\n",
        "import warnings\n",
        "import time\n",
        "import ast\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
        "\n",
        "import scipy.stats as st\n",
        "from tqdm import tqdm, tqdm_pandas\n",
        "tqdm_pandas(tqdm())\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as  pd\n",
        "from pprint import pprint# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
        "import spacy# Plotting tools\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "#Library needed for selecting the number of topics\n",
        "!pip install kneebow\n",
        "from kneebow.rotor import Rotor\n",
        "\n",
        "#Library and initalization  needed to interpret topic composition\n",
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "!databricks configure --host https://community.cloud.databricks.com/\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Shared/base_model_thesis\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WQ1VhlVV6Ol",
        "outputId": "0fd3d26a-0e39-4d1d-89e8-1027de75ba2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (8.1.3)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.1)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.17.7)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.31)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.19.6)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2022.7.1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.27.1)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.1)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.6.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (0.4.4)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.10.4)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (6.1.1)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.4)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.22.4)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.10.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.5.3)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.10)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.10/dist-packages (from mlflow) (20.1.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.5.0)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.15)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow) (1.5.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.3.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.10)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.10/dist-packages (from gunicorn<21->mlflow) (67.7.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_decision_forests==1.2.0 in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (1.5.3)\n",
            "Requirement already satisfied: tensorflow~=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (2.11.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (0.40.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests==1.2.0) (3.0.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.8.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (23.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.32.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests==1.2.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests==1.2.0) (2022.7.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_decision_forests==1.2.0) (3.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TensorFlow Decision Forests 1.2.0 is compatible with the following TensorFlow Versions: ['2.11.0']. However, TensorFlow 2.11.1 was detected. This can cause issues with the TF API and symbols in the custom C++ ops. See the TF and TF-DF compatibility table at https://github.com/tensorflow/decision-forests/blob/main/documentation/known_issues.md#compatibility-table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==4.8.0\n",
            "  Downloading tweepy-4.8.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy==4.8.0) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy==4.8.0) (2.27.1)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy==4.8.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy==4.8.0) (3.4)\n",
            "Installing collected packages: tweepy\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 4.13.0\n",
            "    Uninstalling tweepy-4.13.0:\n",
            "      Successfully uninstalled tweepy-4.13.0\n",
            "Successfully installed tweepy-4.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.0\n",
            "Username: ohauglend@gmail.com\n",
            "Password: \n",
            "Repeat for confirmation: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/2976583595097642', creation_time=1675704450858, experiment_id='2976583595097642', last_update_time=1681327508835, lifecycle_stage='active', name='/Shared/base_model_thesis', tags={'mlflow.experiment.sourceName': '/Shared/base_model_thesis',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'ohauglend@gmail.com',\n",
              " 'mlflow.ownerId': '1309670178152885'}>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCDzeRQgOQVF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Custom_keras_RandomForestModel(tfdf.keras.RandomForestModel):\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "      super().__init__(*args, **kwargs)\n",
        "      self.input_consumer_key = tf.Variable(\"\")\n",
        "      self.input_consumer_secret = tf.Variable(\"\")\n",
        "      self.input_access_token = tf.Variable(\"\")\n",
        "      self.input_access_token_secret = tf.Variable(\"\")\n",
        "      self.input_bearer_token = tf.Variable(\"\")\n",
        "    \n",
        "    @classmethod\n",
        "    def set_Twitter_API_keys(self,\n",
        "                             input_consumer_key,\n",
        "                             input_consumer_secret,\n",
        "                             input_access_token,\n",
        "                             input_access_token_secret,\n",
        "                             input_bearer_token\n",
        "                             ):\n",
        "      self.input_consumer_key = input_consumer_key\n",
        "      self.input_consumer_secret = input_consumer_secret\n",
        "      self.input_access_token = input_access_token\n",
        "      self.input_access_token_secret = input_access_token_secret\n",
        "      self.input_bearer_token = input_bearer_token\n",
        "\n",
        "    @classmethod\n",
        "    def get_Twitter_API_keys(self):\n",
        "      return {'input_consumer_key': self.input_consumer_key,\n",
        "              'input_consumer_secret': self.input_consumer_secret,\n",
        "              'input_access_token': self.input_access_token,\n",
        "              'input_access_token_secret': self.input_access_token_secret,\n",
        "              'input_bearer_token': self.input_bearer_token}\n",
        "\n",
        "    @classmethod\n",
        "    def extract_twitter_data_from_users(self,\n",
        "                                        input_df_path,\n",
        "                                        output_folder_path,\n",
        "                                        RUN_ID,\n",
        "                                        generate_embeddings_file = True,\n",
        "                                        local_repository = '/content/',\n",
        "                                        storing_intervall = 10000,\n",
        "                                        filter_retweets = False,\n",
        "                                        filter_replies = False,\n",
        "                                        conversation_id = None,\n",
        "                                        only_english = True,\n",
        "                                        #requires prior import of datetime.datetime\n",
        "                                        date_since = (datetime.now() - timedelta(days = 7)),\n",
        "                                        date_until = (datetime.now() - timedelta(days = 0)),\n",
        "                                        search_tweets = True,\n",
        "                                        search_30_day = False,\n",
        "                                        search_full_archive = False,\n",
        "                                        search_tweets_result_type = 'recent',\n",
        "                                        search_tweets_tweet_mode = 'extended',\n",
        "                                        search_30_day_label = 'sandbox',\n",
        "                                        search_full_archive_label = 'sandbox',\n",
        "                                        search_tweets_total_queries = 1000000,\n",
        "                                        search_30_day_number_of_days = 30,\n",
        "                                        search_30_day_maxResults = 100,\n",
        "                                        search_full_archive_number_of_days = 20,\n",
        "                                        search_full_archive_maxResults = 100,\n",
        "                                        system_change_input_dynamically = False\n",
        "                                        ):\n",
        "        \n",
        "      \"\"\"\n",
        "      Function to be used for data extraction if there is a known list of users\n",
        "      Input schema (input_df_path) should follow that of '/content/drive/MyDrive/Bot detection/Data/Input_to_servers/Getting_tweet_level_features/Getting_tweet_level_features_split_172_18_0_84.json'\n",
        "      The function has no return\n",
        "      \n",
        "      The intention of the function is to provide large scale real world data extraction to be used for model testing and later retrainging/tuning.\n",
        "      The data collection schema follows that of previous scripts in this project and can largely be divided into three bolks:\n",
        "        - User meta data\n",
        "        - Features derived from Yang.et al \n",
        "        - Tweet level features\n",
        "\n",
        "      The function has no return, but stores the extracted data in the specified output_folder_path.\n",
        "      \n",
        "      The function takes in input_df_path, output_folder_path, and query as only default arguments.\n",
        "\n",
        "        input_df_path is used to gather colums/features that the system should extract from Twitter.\n",
        "          - It is intended to be passed a df that contains user meta data features and features from Yang et al, \n",
        "            however, the system should also be able to handle a df that also contains Tweet based features.\n",
        "        \n",
        "        output_folder_path is the folder where the data should be stored. \n",
        "          - Specify without the final /\n",
        "          - Stores entries at every storing_intervall iteration\n",
        "        \n",
        "        query is the keyword/words used to search Twitter for, for syntax please consult:\n",
        "        - https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md\n",
        "        \n",
        "        Twitter API credentials are set by: - it is assumed that the user will use academic access credentials.\n",
        "          - input_consumer_key\n",
        "          - input_consumer_secret\n",
        "          - input_access_token\n",
        "          - input_access_token_secret\n",
        "          - input_bearer_token\n",
        "\n",
        "      The function takes in several non-default arguments;\n",
        "        \n",
        "        storing_intervall controls the frequency of storage, it is defaulted to store every 10 000 entries, later iterations also contian data from previous iterations.\n",
        "        \n",
        "        Query manipulation is done by:\n",
        "          - filter_retweets - If set to True, retweets are removed from the data collected - Defaulted to False\n",
        "          - filter_replies - If set to True, replies are removed from the data collected - Defaulted to False\n",
        "          - conversation_id - If an id is specified the system will retrieve replies and retweets assosiated with this conversation_id - Defaulted to None\n",
        "          - only_english skips all Tweets that are not in english - Defaulted to True:\n",
        "            - Keep in mind that the base model is only trained on english data\n",
        "        \n",
        "        Date range parameters are controled by:\n",
        "          - date_since - Controls the lower limit of date range taken into account by the system - Defaulted to 7 days before the current day\n",
        "          - date_until - Controls the upper limit of date range taken into account by the system - Defaulted to 0 days before the current day\n",
        "        The system is intended to collect all data from the specified days, meaning a high number of queries should be specified\n",
        "        Date parameters should be passed in as (datetime.now() - timedelta(days = X) - timedelta(hours = Y))\n",
        "        Keep in mind that cursor works badly with hours, meaning the reccomended approach is to set limit parameters high enough that it will collect the whole date range\n",
        "\n",
        "        The remaining parameters controls the collection method used to obtain Twitter data.\n",
        "        - The defaulted one is to use search_tweets as this requires no development environment setup other than normal app credentials. \n",
        "          * This is done by setting search_tweets = True, search_30_day = False, and search_full_archive = False\n",
        "          * search_tweets_result_type controls which Tweets the system prioritizes. It is defaulted to 'recent', possible other values are 'popular' and 'mixed'\n",
        "          * search_tweets_tweet_mode controls how much of the Tweet is returned. It is defaulted to 'extended'\n",
        "          * search_tweets_total_queries controls the total number of queries made to Twitter API. This should be set in conjunction with date parameters and is defaulted to a high 1 000 000. \n",
        "          \n",
        "          search_tweets is the defaulted collection method as it is the most feasible approach without purchasing aditional subscriptions.\n",
        "          Further, it is hypothesised that the most recent data will be the most relevant ones. \n",
        "          Hence, it is recommended to use search_tweets, however if needed the system also supports search_30_day and search_full_archive collection methods.\n",
        "          Other methods require extra development environments, in order to set up see: https://developer.twitter.com/en/account/environments\n",
        "          Keep in mind that max rate limits per call for sandbox environments are currently 500. \n",
        "          If this function is to be used with older data as it can be with search_tweets one therefore, needs to purchase additional subscriptions.\n",
        "\n",
        "        - search_30_day enables the user to search the 30 day archive of Twitter instead of the 7 day defaulted one. \n",
        "          * It requires an environment to be set up within the app, and hence also a label to passed in instantiation, this is controled by the search_30_day_label parameter\n",
        "          * The function loops through dates in backwards order until search_30_day_number_of_days is met, extracting search_30_day_maxResults per day\n",
        "            * Increasing this to > 30 has no effect -> will effectively be 30\n",
        "          * As other (cheaper) methods can handle more recent data, it is recommended to set: date_since = (datetime.now() - timedelta(days = 8)), date_until = (datetime.now() - timedelta(days = 7))\n",
        "          * search_30_day_maxResults controls the number of queries made to the system - possible values: [10,100]\n",
        "          If these cannot be handled as missing values one needs to look up the user directly\n",
        "\n",
        "        - search_full_archive enables the user to search the full archive of Twitter instead of the 7 day defaulted one. \n",
        "          * It requires an environment to be set up within the app, and hence also a label to passed in instantiation, this is controled by the search_full_archive_label parameter\n",
        "          * The function loops through dates in backwards order until search_full_archive_number_of_days is met, extracting search_full_archive_maxResults per day\n",
        "          * As other (cheaper) methods can handle more recent data, it is recommended to set: date_since = (datetime.now() - timedelta(days = 31)), date_until = (datetime.now() - timedelta(days = 30))\n",
        "          * search_full_archive_maxResults controls the number of queries made to the system - possible values: [10,100]\n",
        "          If these cannot be handled as missing values one needs to look up the user directly\n",
        "\n",
        "        - system_change_input_dynamically enables the system to change between search_tweets, search_30_day, and search_full_archive dynamically based on inputted date parameters.\n",
        "          It is defaulted to False.\n",
        "\n",
        "\n",
        "\n",
        "      #imports\n",
        "      import pandas as pd\n",
        "      import re\n",
        "      import numpy as np\n",
        "      import os\n",
        "      from tqdm import tqdm\n",
        "\n",
        "\n",
        "      #Need >= 4.8.0\n",
        "      !pip install tweepy==4.8.0\n",
        "\n",
        "      #Setting up email notification for when errors are triggered\n",
        "      import smtplib \n",
        "      import socket\n",
        "\n",
        "      import tweepy as tw\n",
        "      from tweepy.errors import TooManyRequests\n",
        "      from tweepy.errors import Forbidden, NotFound, TwitterServerError\n",
        "      from tweepy.errors import BadRequest\n",
        "      #NotFound: 404 Not Found\n",
        "\n",
        "      import copy\n",
        "      from datetime import date, timedelta, datetime\n",
        "      import warnings\n",
        "      import time\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      #Validating that set_Twitter_API_keys has been called\n",
        "      #input_access_token_secret is not checked as it can be empty (if only essential access)\n",
        "      if self.input_consumer_key == \"\" or self.input_consumer_secret == \"\" or self.input_access_token == \"\" or self.input_bearer_token == \"\":\n",
        "        raise ValueError(\"Twitter API credentials are empty strings please use model method set_Twitter_API_keys\")\n",
        "      \n",
        "      else:\n",
        "        input_consumer_key= self.input_consumer_key\n",
        "        input_consumer_secret= self.input_consumer_secret\n",
        "        input_access_token= self.input_access_token\n",
        "        input_access_token_secret= self.input_access_token_secret\n",
        "        input_bearer_token= self.input_bearer_token\n",
        "\n",
        "      #Connecting to the api, setting the keys\n",
        "      \n",
        "      try:\n",
        "        auth = tw.OAuthHandler(input_consumer_key, input_consumer_secret)\n",
        "        if(input_access_token_secret!= \"\"):\n",
        "          auth.set_access_token(input_access_token, input_access_token_secret)\n",
        "        api = tw.API(auth, wait_on_rate_limit=True)\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "      count = 0\n",
        "\n",
        "      ##\n",
        "      input_df = pd.read_json(input_df_path)\n",
        "      input_df = input_df.reset_index(drop = True)\n",
        "      df = input_df\n",
        "\n",
        "\n",
        "\n",
        "      entry_lst = []\n",
        "\n",
        "      \n",
        "      #Input validation\n",
        "      if date_until > datetime.now():\n",
        "        raise ValueError(\"date_until is after the present date\")\n",
        "        \n",
        "      if date_since > date_until:\n",
        "        raise ValueError(\"date_since is after date_until, the specified start range is after the specified end range\")\n",
        "\n",
        "      if sum([search_tweets, search_30_day, search_full_archive]) >= 2:\n",
        "        raise ValueError(\"The Boolean parameters search_tweets, search_30_day, search_full_archive are used to select between different cursors, please only specify one, for usecase see docstring\")\n",
        "      \n",
        "      #As we would like a lean approach - search_full_archive is more expensive than search_30_day which is more expensive than search_tweets,\n",
        "      #The system can change implementation based on input feasibility, a warning is also thrown, it can be toggled by the system_change_input_dynamically parameter \n",
        "      if system_change_input_dynamically:\n",
        "        if (datetime.now() - date_since).days <= 7 and (datetime.now() - date_until).days <= 7:\n",
        "          search_tweets = True\n",
        "          search_30_day = False\n",
        "          search_full_archive = False\n",
        "          warnings.warn('System changed implementation to serch_tweets based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "\n",
        "        elif (datetime.now() - date_since).days <= 30 and (datetime.now() - date_until).days <= 30:\n",
        "          search_tweets = False\n",
        "          search_30_day = True\n",
        "          search_full_archive = False\n",
        "          warnings.warn('System changed implementation to search_30_day based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "        \n",
        "        else:\n",
        "          search_tweets = False\n",
        "          search_30_day = False\n",
        "          search_full_archive = True\n",
        "          warnings.warn('System changed implementation to search_full_archive based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "\n",
        "      \n",
        "      #looping thorugh columns and clearing content in df except for do_not_clear_columns_lst \n",
        "      do_not_clear_columns_lst = ['user_screen_name', 'Bot', 'Source/label']\n",
        "      for col in df.columns.tolist():\n",
        "        if col in do_not_clear_columns_lst:\n",
        "          continue\n",
        "        if type(df[col].iloc[0]) == np.float64:\n",
        "          df[col] = [np.nan for _ in range(len(df))]\n",
        "        else:\n",
        "          df[col] = ['' for _ in range(len(df))]\n",
        "\n",
        "      #Specifying tweet specific variables\n",
        "      df['tweet_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_created_at'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_text'] = ['' for _ in range(len(df))]\n",
        "      df['length_of_text'] = [np.nan for _ in range(len(df))]\n",
        "      df['is_retweet'] = [np.nan  for _ in range(len(df))]\n",
        "      df['is_reply'] = [np.nan  for _ in range(len(df))]\n",
        "      df['tweet_mentions_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_mentions_user_screen_name'] = [np.nan for _ in range(len(df))]\n",
        "      df['quote_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['number_of_tweet_mentions'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweet_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['is_retweeted'] = [np.nan for _ in range(len(df))]\n",
        "      df['favorite_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['contains_media'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_types'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_source_status_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_original_source_status_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_source_user_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_original_source_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['source'] = ['' for _ in range(len(df))]\n",
        "\n",
        "      #Too few entries to be used as features, but can be used for later network analysis\n",
        "      df['retweeted_status_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweeted_status_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweeted_status_user_screen_name'] = ['' for _ in range(len(df))]\n",
        "      df['in_reply_to_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['in_reply_to_status_id'] = [np.nan for _ in range(len(df))]\n",
        "\n",
        "      #Setting column types that can hold lists for relevant columns\n",
        "      for col in ['tweet_mentions_user_id',\n",
        "                  'tweet_mentions_user_screen_name',\n",
        "                  'media_ids',\n",
        "                  'media_types',\n",
        "                  'media_source_status_ids',\n",
        "                  'media_source_user_ids',\n",
        "                  'media_original_source_status_id',\n",
        "                  'media_original_source_user_id']:\n",
        "              df[col] = df[col].astype(object)\n",
        "\n",
        "      #Wrapping it in a try loop so features can be derived from what is collected in case there is an error\n",
        "      for user_screen_name in tqdm(df.user_screen_name.tolist()):\n",
        "        query = 'from:'+user_screen_name\n",
        "        #Defining collection method based on input\n",
        "        try:\n",
        "          if search_tweets:\n",
        "            for entry in tw.Cursor(api.search_tweets,\n",
        "                                          q=query,\n",
        "                                          since=date_since.strftime('%Y-%m-%d'),\n",
        "                                          until=date_until.strftime('%Y-%m-%d'),\n",
        "                                          result_type  = search_tweets_result_type,\n",
        "                                          tweet_mode=search_tweets_tweet_mode).items(search_tweets_total_queries):\n",
        "\n",
        "\n",
        "              #Storring entry_lst at storing_intervall\n",
        "              if len(entry_lst) % storing_intervall == 0 and len(entry_lst) != 0:\n",
        "                pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                    \n",
        "              #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "              entry_lst.append(entry)\n",
        "                    \n",
        "              #Having the system sleep between each entry to limit the load on Twitter API\n",
        "              time.sleep(0.5)\n",
        "\n",
        "                    \n",
        "\n",
        "          elif search_30_day:\n",
        "\n",
        "            for i in range(search_30_day_number_of_days): \n",
        "\n",
        "              try:\n",
        "                for entry in api.search_30_day(query = query, \n",
        "                                          label = search_30_day_label,\n",
        "                                          fromDate = (date_since -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                          toDate = (date_until -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                          maxResults = search_30_day_maxResults\n",
        "                                          ):\n",
        "\n",
        "                  #Storring entry_lst at storing_intervall\n",
        "                  if len(entry_lst) % storing_intervall == 0 and len(entry_lst) != 0:\n",
        "                    pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                    print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                        \n",
        "                  #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "                  entry_lst.append(entry)\n",
        "                        \n",
        "                  #Having the system sleep between each entry to limit the load on Twitter API\n",
        "                  time.sleep(0.5)\n",
        "              except:\n",
        "                  pass\n",
        "\n",
        "\n",
        "          elif search_full_archive:\n",
        "\n",
        "            for i in range(search_full_archive_number_of_days):\n",
        "              \n",
        "              try:\n",
        "                \n",
        "                for entry in api.search_full_archive(query = query, \n",
        "                                          label = search_full_archive_label,\n",
        "                                          fromDate = (date_since -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                          toDate = (date_until -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                          maxResults = search_full_archive_maxResults\n",
        "                                          ):\n",
        "                  \n",
        "                  #Storring entry_lst at storing_intervall\n",
        "                  if len(entry_lst) % storing_intervall == 0 and len(entry_lst) != 0:\n",
        "                    pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                    print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                        \n",
        "                  #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "                  entry_lst.append(entry)\n",
        "                        \n",
        "                  #Having the system sleep between each entry to limit the load on Twitter API\n",
        "                  time.sleep(0.5)\n",
        "            \n",
        "              except:\n",
        "                pass\n",
        "\n",
        "                        \n",
        "      \n",
        "        except Exception as e:\n",
        "\n",
        "            #Storring entry_lst\n",
        "            pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "            print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "\n",
        "            #Sending oha.digi@cbs.dk an email if exception is triggered\n",
        "            #Getting local IP\n",
        "            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "            s.connect((\"8.8.8.8\", 80))\n",
        "            LOCAL_IP = s.getsockname()[0]\n",
        "            s.close()\n",
        "\n",
        "            #Setting up connection to my gmail\n",
        "            with smtplib.SMTP_SSL('smtp.gmail.com', 465) as connection:  \n",
        "                email_address = 'ohauglend@gmail.com'\n",
        "                email_password = 'xoughywacvkfmkhi'\n",
        "                connection.login(email_address, email_password )\n",
        "                connection.sendmail(from_addr=email_address, to_addrs='oha.digi@cbs.dk',\n",
        "                                    msg=\"IP: \"+str(LOCAL_IP)+\"\\nextract_twitter_data Cursor failed, Error: \\t\"+ str(e) + '\\nNumber of entries in Cursor:\\t' +str(len(entry_lst)))\n",
        "            pass\n",
        "\n",
        "\n",
        "      #Storing entry_lst in case something happens in populating df\n",
        "      pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst.json\")\n",
        "      print('entry_lst stored at:\\t'+str(output_folder_path)+\"/entry_lst.json\")\n",
        "\n",
        "\n",
        "      \n",
        "      #looping through entry_lst to populate input_df\n",
        "      for i in tqdm(range(len(entry_lst))):\n",
        "\n",
        "\n",
        "            #Storing input_df in output_folder_path as a .json file with the user_id of the last user to have features populated\n",
        "            #Not storing on the first iteration of input_df\n",
        "            if i % storing_intervall == 0 and i != 0:\n",
        "                input_df.to_json(output_folder_path+\"/last_user_id_\"+str(input_df.user_id.iloc[i-1])+\".json\")\n",
        "                print(\"File stored at: \"+output_folder_path+\"/last_user_id_\"+str(input_df.user_id.iloc[i-1])+\".json\")\n",
        "        \n",
        "            #Adding user specific features to the input_df\n",
        "            #When storing input_df to json the date will be casted to the correct format\n",
        "            input_df.at[i, 'created_at_1'] = entry_lst[i].user.created_at\n",
        "            input_df.at[i, 'user_id'] = entry_lst[i].user.id\n",
        "            input_df.at[i, 'user_name'] = entry_lst[i].user.name\n",
        "            input_df.at[i, 'user_screen_name'] = entry_lst[i].user.screen_name\n",
        "            input_df.at[i,'user_location'] = entry_lst[i].user.location\n",
        "\n",
        "            #user_description \n",
        "            if type(entry_lst[i].user.description) == str:\n",
        "                input_df.at[i, 'user_description'] = entry_lst[i].user.description\n",
        "\n",
        "\n",
        "            #user_url \n",
        "            if type(entry_lst[i].user.url) == str:\n",
        "                input_df.at[i,'user_url'] = entry_lst[i].user.url\n",
        "\n",
        "        \n",
        "            #protected \n",
        "            if type(entry_lst[i].user.protected) == bool:\n",
        "                input_df.at[i,'protected'] = float(entry_lst[i].user.protected)\n",
        "\n",
        "\n",
        "            #followers_count \n",
        "            if type(entry_lst[i].user.followers_count) == int:\n",
        "                input_df.at[i,'followers_count'] = float(entry_lst[i].user.followers_count)\n",
        "\n",
        "        \n",
        "            #friends_count \n",
        "            if type(entry_lst[i].user.friends_count) == int:\n",
        "                input_df.at[i,'friends_count'] = float(entry_lst[i].user.friends_count)\n",
        "\n",
        "\n",
        "            #listed_count \n",
        "            if type(entry_lst[i].user.listed_count) == int:\n",
        "                input_df.at[i,'listed_count'] = float(entry_lst[i].user.listed_count)\n",
        "\n",
        "\n",
        "\n",
        "            #favourites_count \n",
        "            if type(entry_lst[i].user.favourites_count) == int:\n",
        "                input_df.at[i,'favourites_count'] = float(entry_lst[i].user.favourites_count)\n",
        "\n",
        "\n",
        "            #utc_offset \n",
        "            #missing values are marked as np.nan meaning comparing on type (float) does not work\n",
        "            if type(entry_lst[i].user.utc_offset) == str:\n",
        "                input_df.at[i,'utc_offset'] = entry_lst[i].user.utc_offset\n",
        "\n",
        "\n",
        "            #time_zone \n",
        "            if type(entry_lst[i].user.time_zone) == str:\n",
        "                input_df.at[i,'time_zone'] = entry_lst[i].user.time_zone\n",
        "\n",
        "\n",
        "            #geo_enabled \n",
        "            if type(entry_lst[i].user.geo_enabled) == bool:\n",
        "                input_df.at[i,'geo_enabled'] = float(entry_lst[i].user.geo_enabled)\n",
        "\n",
        "\n",
        "            #verified - might change - for now: all acounts except 3 in dataset have binary labels if verified or not :\n",
        "            if type(entry_lst[i].user.verified) == bool:\n",
        "                input_df.at[i,'verified'] = float(entry_lst[i].user.verified)\n",
        "\n",
        "          \n",
        "            #statuses_count -update if new\n",
        "            if type(entry_lst[i].user.statuses_count) == int:\n",
        "                input_df.at[i,'statuses_count'] = float(entry_lst[i].user.statuses_count)\n",
        "\n",
        "\n",
        "            #lang \n",
        "            if type(entry_lst[i].user.lang) == str:\n",
        "                input_df.at[i,'lang'] = entry_lst[i].user.lang\n",
        "\n",
        "\n",
        "            #contributors_enabled \n",
        "            if type(entry_lst[i].user.contributors_enabled) == bool:\n",
        "                input_df.at[i,'contributors_enabled'] = float(entry_lst[i].user.contributors_enabled)\n",
        "\n",
        "\n",
        "            #is_translator \n",
        "            if type(entry_lst[i].user.is_translator) == bool:\n",
        "                input_df.at[i,'is_translator'] = float(entry_lst[i].user.is_translator)\n",
        "\n",
        "\n",
        "            #profile_background_color \n",
        "            if type(entry_lst[i].user.profile_background_color) == str:\n",
        "                input_df.at[i,'profile_background_color'] = entry_lst[i].user.profile_background_color\n",
        "\n",
        "\n",
        "            #profile_background_image_url \n",
        "            if type(entry_lst[i].user.profile_background_image_url) == str:\n",
        "                input_df.at[i,'profile_background_image_url'] = entry_lst[i].user.profile_background_image_url\n",
        "\n",
        "\n",
        "            #profile_background_image_url_https \n",
        "            if type(entry_lst[i].user.profile_background_image_url_https) == str:\n",
        "                input_df.at[i,'profile_background_image_url_https'] = entry_lst[i].user.profile_background_image_url_https\n",
        "\n",
        "\n",
        "            #profile_background_tile \n",
        "            if type(entry_lst[i].user.profile_background_tile) == bool:\n",
        "                input_df.at[i,'profile_background_tile'] = float(entry_lst[i].user.profile_background_tile)\n",
        "\n",
        "\n",
        "            #profile_image_url \n",
        "            if type(entry_lst[i].user.profile_image_url) == str:\n",
        "                input_df.at[i,'profile_image_url'] = entry_lst[i].user.profile_image_url\n",
        "          \n",
        "\n",
        "            #profile_image_url_https \n",
        "            if type(entry_lst[i].user.profile_image_url_https) == str:\n",
        "                input_df.at[i,'profile_image_url_https'] = entry_lst[i].user.profile_image_url_https\n",
        "\n",
        "\n",
        "            #profile_link_color \n",
        "            if type(entry_lst[i].user.profile_link_color) == str:\n",
        "                input_df.at[i,'profile_link_color'] = entry_lst[i].user.profile_link_color\n",
        "\n",
        "\n",
        "            #profile_sidebar_border_color \n",
        "            if type(entry_lst[i].user.profile_sidebar_border_color) == str:\n",
        "                input_df.at[i,'profile_sidebar_border_color'] = entry_lst[i].user.profile_sidebar_border_color\n",
        "\n",
        "\n",
        "            #profile_sidebar_fill_color \n",
        "            if type(entry_lst[i].user.profile_sidebar_fill_color) == str:\n",
        "                input_df.at[i,'profile_sidebar_fill_color'] = entry_lst[i].user.profile_sidebar_fill_color\n",
        "\n",
        "\n",
        "            #profile_text_color \n",
        "            if type(entry_lst[i].user.profile_text_color) == str:\n",
        "                input_df.at[i,'profile_text_color'] = entry_lst[i].user.profile_text_color\n",
        "          \n",
        "\n",
        "            #profile_use_background_image \n",
        "            if type(entry_lst[i].user.profile_use_background_image) == bool:\n",
        "                input_df.at[i,'profile_use_background_image'] = float(entry_lst[i].user.profile_use_background_image)\n",
        "\n",
        "\n",
        "            #translator_type \n",
        "            if type(entry_lst[i].user.translator_type) == str:\n",
        "                input_df.at[i,'translator_type'] = entry_lst[i].user.translator_type\n",
        "\n",
        "\n",
        "            #withheld \n",
        "            if type(entry_lst[i].user.withheld_in_countries) == list:\n",
        "                input_df.at[i,'withheld'] = entry_lst[i].user.withheld_in_countries\n",
        "\n",
        "            #default_profile \n",
        "            if type(entry_lst[i].user.default_profile) == bool:\n",
        "                input_df.at[i,'default_profile'] = float(entry_lst[i].user.default_profile)\n",
        "\n",
        "            #default_profile_image \n",
        "            if type(entry_lst[i].user.default_profile_image) == bool:\n",
        "                input_df.at[i,'default_profile_image'] = float(entry_lst[i].user.default_profile_image)\n",
        "        \n",
        "\n",
        "\n",
        "            #Adding the Tweet specific features to input_df\n",
        "            input_df.at[i, 'tweet_id'] = entry_lst[i].id\n",
        "            input_df.at[i, 'tweet_created_at'] = entry_lst[i].created_at\n",
        "        \n",
        "            if search_tweets:\n",
        "                input_df.at[i, 'tweet_text'] = entry_lst[i].full_text\n",
        "                input_df.at[i, 'length_of_text'] = len(entry_lst[i].full_text)\n",
        "\n",
        "            else:\n",
        "                input_df.at[i, 'tweet_text'] = entry_lst[i].text\n",
        "                input_df.at[i, 'length_of_text'] = len(entry_lst[i].text)\n",
        "\n",
        "\n",
        "        \n",
        "            try:\n",
        "                entry_lst[i].retweeted_status\n",
        "                input_df.at[i, 'is_retweet'] = float(True)\n",
        "                \n",
        "                #Although these are too few entries to be used as features, they can be useful for later network-based approaches\n",
        "                input_df.at[i, 'retweeted_status_id'] = entry_lst[i].retweeted_status.id\n",
        "                input_df.at[i, 'retweeted_status_user_id'] = entry_lst[i].retweeted_status.user.id\n",
        "                input_df.at[i, 'retweeted_status_user_screen_name'] = entry_lst[i].retweeted_status.user.screen_name\n",
        "\n",
        "\n",
        "            except AttributeError:\n",
        "                input_df.at[i, 'is_retweet'] = float(False)\n",
        "\n",
        "            try:\n",
        "                entry_lst[i].in_reply_to_status_id\n",
        "                input_df.at[i, 'is_reply'] = float(True)\n",
        "                \n",
        "                #Although these are too few entries to be used as features, they can be useful for later network-based approaches\n",
        "                input_df.at[i, 'in_reply_to_status_id'] = entry_lst[i].in_reply_to_status_id\n",
        "                input_df.at[i, 'in_reply_to_user_id'] = entry_lst[i].in_reply_to_user_id\n",
        "\n",
        "\n",
        "            except:\n",
        "                input_df.at[i, 'is_reply'] = float(False)\n",
        "                \n",
        "                input_df.at[i, 'tweet_mentions_user_id'] = [entry_lst[i].entities['user_mentions'][_]['id'] for _ in range(len(entry_lst[i].entities['user_mentions']))]\n",
        "                input_df.at[i, 'tweet_mentions_user_screen_name'] = [entry_lst[i].entities['user_mentions'][_]['screen_name'] for _ in range(len(entry_lst[i].entities['user_mentions']))]\n",
        "                input_df.at[i, 'number_of_tweet_mentions'] = len(entry_lst[i].entities['user_mentions'])\n",
        "\n",
        "            try:\n",
        "                input_df.at[i, 'quote_count'] = entry_lst[i].quote_count\n",
        "            except AttributeError:\n",
        "                input_df.at[i, 'quote_count'] = 0\n",
        "\n",
        "        \n",
        "            input_df.at[i, 'retweet_count'] = entry_lst[i].retweet_count\n",
        "            if (entry_lst[i].retweet_count == 0):\n",
        "                input_df.at[i, 'is_retweeted'] = float(True)\n",
        "            else:\n",
        "                input_df.at[i, 'is_retweeted'] = float(False)\n",
        "\n",
        "        \n",
        "            try:\n",
        "                input_df.at[i, 'favorite_count'] = entry_lst[i].favorite_count\n",
        "            except AttributeError:\n",
        "                input_df.at[i, 'favorite_count'] = 0\n",
        "\n",
        "            try:\n",
        "              entry_lst[i].extended_entities['media']\n",
        "              input_df.at[i, 'contains_media'] = float(True)\n",
        "              input_df.at[i, 'media_ids'] = [entry_lst[i].extended_entities['media'][_].get('id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "              input_df.at[i, 'media_types'] = [entry_lst[i].extended_entities['media'][_].get('type') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "              \n",
        "              if [entry_lst[i].extended_entities['media'][_].get('source_status_id') for _ in range(len(entry_lst[i].extended_entities['media']))] != [None]:\n",
        "                    input_df.at[i, 'media_source_status_ids'] = [entry_lst[i].extended_entities['media'][_].get('source_status_id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "                    input_df.at[i, 'media_original_source_status_id'] = [entry_lst[i].extended_entities['media'][_].get('source_status_id') == entry_lst[i].id for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "                    \n",
        "              if [entry_lst[i].extended_entities['media'][_].get('source_user_id') for _ in range(len(entry_lst[i].extended_entities['media']))] != [None]:\n",
        "                    input_df.at[i, 'media_source_user_ids'] = [entry_lst[i].extended_entities['media'][_].get('source_user_id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "                    input_df.at[i, 'media_original_source_user_id'] = [entry_lst[i].extended_entities['media'][_].get('source_user_id') == entry_lst[i].user.id for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "            except:\n",
        "              input_df.at[i, 'contains_media'] = float(False)\n",
        "\n",
        "        \n",
        "            input_df.at[i,'source'] = entry_lst[i].source\n",
        "\n",
        "\n",
        "            #Specifying yang et al features\n",
        "            try:\n",
        "                #Getting user age as a variable to compute the ratios - also storing user age in the input_df but it might not be used as a feature\n",
        "                #The storing of created_at values takes different formats in some cases, either a timestamp or an integer\n",
        "                try:\n",
        "                    user_age = int(time.mktime(input_df.tweet_created_at.iloc[i].timetuple())) - int(time.mktime(input_df.created_at_1.iloc[i].timetuple()))\n",
        "                except ValueError:\n",
        "                    user_age = int(input_df.tweet_created_at.iloc[i]) - int(input_df.created_at_1.iloc[i])\n",
        "                input_df.at[i, 'user_age'] = user_age\n",
        "                input_df.at[i, 'tweet_frequency'] = input_df.statuses_count.iloc[i]/user_age\n",
        "                input_df.at[i, 'followers_growth_rate'] = input_df.followers_count.iloc[i]/user_age\n",
        "                input_df.at[i, 'friends_growth_rate'] = input_df.friends_count.iloc[i]/user_age\n",
        "                input_df.at[i, 'favourites_growth_rate'] = input_df.favourites_count.iloc[i]/user_age\n",
        "                input_df.at[i, 'listed_growth_rate'] = input_df.listed_count.iloc[i]/user_age\n",
        "                \n",
        "                #Poulating features non-dependent on age\n",
        "                input_df.at[i, 'followers_friends_ratio'] = input_df.followers_count.iloc[i]/input_df.friends_count.iloc[i]\n",
        "                input_df.at[i, 'screen_name_length'] = len(input_df.user_screen_name.iloc[i])\n",
        "                input_df.at[i, 'num_digits_in_screen_name'] = sum(c.isdigit() for c in input_df.user_screen_name.iloc[i])\n",
        "                input_df.at[i, 'name_length'] = len(input_df.user_name.iloc[i])\n",
        "                input_df.at[i, 'num_digits_in_name'] = sum(c.isdigit() for c in input_df.user_name.iloc[i])\n",
        "                input_df.at[i, 'description_length'] = len(input_df.user_description.iloc[i])\n",
        "        \n",
        "            except TypeError as e:\n",
        "                print(\"Exception in Yang et al features, triggered by user:\\t\"+str(input_df.user_id.iloc[i])+\"\\nerror:\\t\"+str(e))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #Storing one final copy after exiting the for loop\n",
        "      input_df.to_json(output_folder_path+\"/FINAL_last_user_id_\"+str(input_df.user_id.iloc[i])+\".json\")\n",
        "      print(\"Twitter data user search file stored at: \"+output_folder_path+\"/FINAL_last_user_id_\"+str(input_df.user_id.iloc[i])+\".json\")\n",
        "\n",
        "      data_path = output_folder_path+\"/FINAL_last_user_id_\"+str(input_df.user_id.iloc[i])+\".json\"\n",
        "\n",
        "      #Generating embeddings file based on RUN_ID, and specified output_folder_path, using path to FINAL_last_user_id file as input_df_path\n",
        "      if generate_embeddings_file:\n",
        "        embeddings_path = self.generate_embeddings(RUN_ID = RUN_ID,\n",
        "                                 input_df_path = output_folder_path+\"/FINAL_last_user_id_\"+str(input_df.user_id.iloc[i])+\".json\",\n",
        "                                 output_folder_path = output_folder_path,\n",
        "                                 local_repository = local_repository)\n",
        "        \n",
        "        return data_path, embeddings_path\n",
        "\n",
        "\n",
        "      return data_path\n",
        "      \n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def extract_twitter_data_keyword(self,\n",
        "                                     input_mock_df_path,\n",
        "                                     output_folder_path,\n",
        "                                     query,\n",
        "                                     RUN_ID,\n",
        "                                     generate_embeddings_file = True,\n",
        "                                     local_repository = '/content/',\n",
        "                                     storing_intervall = 10000,\n",
        "                                     filter_retweets = False,\n",
        "                                     filter_replies = False,\n",
        "                                     conversation_id = None,\n",
        "                                     only_english = True,\n",
        "                                     date_since = (datetime.now() - timedelta(days = 7)),\n",
        "                                     date_until = (datetime.now() - timedelta(days = 0)),\n",
        "                                     search_tweets = True,\n",
        "                                     search_30_day = False,\n",
        "                                     search_full_archive = False,\n",
        "                                     search_tweets_result_type = 'recent',\n",
        "                                     search_tweets_tweet_mode = 'extended',\n",
        "                                     search_30_day_label = 'sandbox',\n",
        "                                     search_full_archive_label = 'sandbox',\n",
        "                                     search_tweets_total_queries = 1000000,\n",
        "                                     search_30_day_number_of_days = 30,\n",
        "                                     search_30_day_maxResults = 100,\n",
        "                                     search_full_archive_number_of_days = 20,\n",
        "                                     search_full_archive_maxResults = 100,\n",
        "                                     system_change_input_dynamically = False,\n",
        "                                     ):\n",
        "      \n",
        "  \n",
        "      \"\"\"\n",
        "      This function is intended to be used for searching for domain specific data\n",
        "      input_mock_df_path should follow the schema of '/content/drive/MyDrive/Bot detection/Data/Input_to_servers/Feature_engineering_not_found_in_output_from_servers_When_working_on_updating_user_ meta/Yang_2020/172_18_0_84_all_data_after_first_crash_of_yang.json'\n",
        "      The function has no return\n",
        "\n",
        "      The intention of the function is to provide large scale real world data extraction to be used for model testing and later retrainging/tuning.\n",
        "      The data collection schema follows that of previous scripts in this project and can largely be divided into three bolks:\n",
        "        - User meta data\n",
        "        - Features derived from Yang.et al \n",
        "        - Tweet level features\n",
        "\n",
        "      The function has no return, but stores the extracted data in the specified output_folder_path.\n",
        "      \n",
        "      The function takes in input_mock_df_path, output_folder_path, and query as only default arguments.\n",
        "\n",
        "        input_mock_df_path is used to gather colums/features that the system should extract from Twitter.\n",
        "          - It is intended to be passed a df that contains user meta data features and features from Yang et al, \n",
        "            however, the system should also be able to handle a df that also contains Tweet based features.\n",
        "        \n",
        "        output_folder_path is the folder where the data should be stored. \n",
        "          - Specify without the final /\n",
        "          - Stores entries at every storing_intervall iteration\n",
        "        \n",
        "        query is the keyword/words used to search Twitter for, for syntax please consult:\n",
        "        - https://github.com/twitterdev/getting-started-with-the-twitter-api-v2-for-academic-research/blob/main/modules/5-how-to-write-search-queries.md\n",
        "        \n",
        "        Twitter API credentials are set by: - it is assumed that the user will use academic access credentials.\n",
        "          - input_consumer_key\n",
        "          - input_consumer_secret\n",
        "          - input_access_token\n",
        "          - input_access_token_secret\n",
        "          - input_bearer_token\n",
        "\n",
        "      The function takes in several non-default arguments;\n",
        "        \n",
        "        storing_intervall controls the frequency of storage, it is defaulted to store every 10 000 entries, later iterations also contian data from previous iterations.\n",
        "        \n",
        "        Query manipulation is done by:\n",
        "          - filter_retweets - If set to True, retweets are removed from the data collected - Defaulted to False\n",
        "          - filter_replies - If set to True, replies are removed from the data collected - Defaulted to False\n",
        "          - conversation_id - If an id is specified the system will retrieve replies and retweets assosiated with this conversation_id - Defaulted to None\n",
        "          - only_english skips all Tweets that are not in english - Defaulted to True:\n",
        "            - Keep in mind that the base model is only trained on english data\n",
        "        \n",
        "        Date range parameters are controled by:\n",
        "          - date_since - Controls the lower limit of date range taken into account by the system - Defaulted to 7 days before the current day\n",
        "          - date_until - Controls the upper limit of date range taken into account by the system - Defaulted to 0 days before the current day\n",
        "        The system is intended to collect all data from the specified days, meaning a high number of queries should be specified\n",
        "        Date parameters should be passed in as (datetime.now() - timedelta(days = X) - timedelta(hours = Y))\n",
        "        Keep in mind that cursor works badly with hours, meaning the reccomended approach is to set limit parameters high enough that it will collect the whole date range\n",
        "\n",
        "        The remaining parameters controls the collection method used to obtain Twitter data.\n",
        "        - The defaulted one is to use search_tweets as this requires no development environment setup other than normal app credentials. \n",
        "          * This is done by setting search_tweets = True, search_30_day = False, and search_full_archive = False\n",
        "          * search_tweets_result_type controls which Tweets the system prioritizes. It is defaulted to 'recent', possible other values are 'popular' and 'mixed'\n",
        "          * search_tweets_tweet_mode controls how much of the Tweet is returned. It is defaulted to 'extended'\n",
        "          * search_tweets_total_queries controls the total number of queries made to Twitter API. This should be set in conjunction with date parameters and is defaulted to a high 1 000 000. \n",
        "          \n",
        "          search_tweets is the defaulted collection method as it is the most feasible approach without purchasing aditional subscriptions.\n",
        "          Further, it is hypothesised that the most recent data will be the most relevant ones. \n",
        "          Hence, it is recommended to use search_tweets, however if needed the system also supports search_30_day and search_full_archive collection methods.\n",
        "          Other methods require extra development environments, in order to set up see: https://developer.twitter.com/en/account/environments\n",
        "          Keep in mind that max rate limits per call for sandbox environments are currently 500. \n",
        "          If this function is to be used with older data as it can be with search_tweets one therefore, needs to purchase additional subscriptions.\n",
        "\n",
        "        - search_30_day enables the user to search the 30 day archive of Twitter instead of the 7 day defaulted one. \n",
        "          * It requires an environment to be set up within the app, and hence also a label to passed in instantiation, this is controled by the search_30_day_label parameter\n",
        "          * The function loops through dates in backwards order until search_30_day_number_of_days is met, extracting search_30_day_maxResults per day\n",
        "            * Increasing this to > 30 has no effect -> will effectively be 30\n",
        "          * As other (cheaper) methods can handle more recent data, it is recommended to set: date_since = (datetime.now() - timedelta(days = 8)), date_until = (datetime.now() - timedelta(days = 7))\n",
        "          * search_30_day_maxResults controls the number of queries made to the system - possible values: [10,100]\n",
        "          If these cannot be handled as missing values one needs to look up the user directly\n",
        "\n",
        "        - search_full_archive enables the user to search the full archive of Twitter instead of the 7 day defaulted one. \n",
        "          * It requires an environment to be set up within the app, and hence also a label to passed in instantiation, this is controled by the search_full_archive_label parameter\n",
        "          * The function loops through dates in backwards order until search_full_archive_number_of_days is met, extracting search_full_archive_maxResults per day\n",
        "          * As other (cheaper) methods can handle more recent data, it is recommended to set: date_since = (datetime.now() - timedelta(days = 31)), date_until = (datetime.now() - timedelta(days = 30))\n",
        "          * search_full_archive_maxResults controls the number of queries made to the system - possible values: [10,100]\n",
        "          If these cannot be handled as missing values one needs to look up the user directly\n",
        "\n",
        "        - system_change_input_dynamically enables the system to change between search_tweets, search_30_day, and search_full_archive dynamically based on inputted date parameters.\n",
        "          It is defaulted to False.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #imports\n",
        "      import pandas as pd\n",
        "      import re\n",
        "      import numpy as np\n",
        "      import os\n",
        "      from tqdm import tqdm\n",
        "\n",
        "\n",
        "      #Need >= 4.8.0\n",
        "      !pip install tweepy==4.8.0\n",
        "\n",
        "      #Setting up email notification for when errors are triggered\n",
        "      import smtplib \n",
        "      import socket\n",
        "\n",
        "      import tweepy as tw\n",
        "      from tweepy.errors import TooManyRequests\n",
        "      from tweepy.errors import Forbidden, NotFound, TwitterServerError\n",
        "      from tweepy.errors import BadRequest\n",
        "      #NotFound: 404 Not Found\n",
        "\n",
        "      import copy\n",
        "      from datetime import date, timedelta, datetime\n",
        "      import warnings\n",
        "      import time\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      #Validating that set_Twitter_API_keys has been called\n",
        "      #input_access_token_secret is not checked as it can be empty (if only essential access)\n",
        "      if self.input_consumer_key == \"\" or self.input_consumer_secret == \"\" or self.input_access_token == \"\" or self.input_bearer_token == \"\":\n",
        "        raise ValueError(\"Twitter API credentials are empty strings please use model method set_Twitter_API_keys\")\n",
        "      \n",
        "      else:\n",
        "        input_consumer_key= self.input_consumer_key\n",
        "        input_consumer_secret= self.input_consumer_secret\n",
        "        input_access_token= self.input_access_token\n",
        "        input_access_token_secret= self.input_access_token_secret\n",
        "        input_bearer_token= self.input_bearer_token\n",
        "\n",
        "\n",
        "      #Connecting to the api, setting the keys      \n",
        "      auth = tw.OAuthHandler(input_consumer_key, input_consumer_secret)\n",
        "      if(input_access_token_secret!= \"\"):\n",
        "        auth.set_access_token(input_access_token, input_access_token_secret)\n",
        "      api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "      \n",
        "      count = 0\n",
        "\n",
        "      df = pd.read_json(input_mock_df_path)\n",
        "\n",
        "\n",
        "      entry_lst = []\n",
        "\n",
        "      \n",
        "      #Input validation\n",
        "      if date_until > datetime.now():\n",
        "        raise ValueError(\"date_until is after the present date\")\n",
        "        \n",
        "      if date_since > date_until:\n",
        "        raise ValueError(\"date_since is after date_until, the specified start range is after the specified end range\")\n",
        "\n",
        "      if sum([search_tweets, search_30_day, search_full_archive]) >= 2:\n",
        "        raise ValueError(\"The Boolean parameters search_tweets, search_30_day, search_full_archive are used to select between different cursors, please only specify one, for usecase see docstring\")\n",
        "      \n",
        "      #As we would like a lean approach - search_full_archive is more expensive than search_30_day which is more expensive than search_tweets,\n",
        "      #The system can change implementation based on input feasibility, a warning is also thrown, it can be toggled by the system_change_input_dynamically parameter \n",
        "      if system_change_input_dynamically:\n",
        "        if (datetime.now() - date_since).days <= 7 and (datetime.now() - date_until).days <= 7:\n",
        "          search_tweets = True\n",
        "          search_30_day = False\n",
        "          search_full_archive = False\n",
        "          warnings.warn('System changed implementation to serch_tweets based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "\n",
        "        elif (datetime.now() - date_since).days <= 30 and (datetime.now() - date_until).days <= 30:\n",
        "          search_tweets = False\n",
        "          search_30_day = True\n",
        "          search_full_archive = False\n",
        "          warnings.warn('System changed implementation to search_30_day based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "        \n",
        "        else:\n",
        "          search_tweets = False\n",
        "          search_30_day = False\n",
        "          search_full_archive = True\n",
        "          warnings.warn('System changed implementation to search_full_archive based on inputted date range\\nThis can be toggled of by setting system_change_input_dynamically to False')\n",
        "\n",
        "\n",
        "      #Query manipulation\n",
        "      query = query.lower()\n",
        "      query = query.replace(' or ',' ') \n",
        "      query = query.replace(' and ',' ') \n",
        "\n",
        "      #Allowing the user to specify the conversation_id as the system defaults to the first one retrived matching the query\n",
        "      if conversation_id == None:\n",
        "      #Removing cases and and/or from input:\n",
        "        if filter_retweets:\n",
        "          query += \" AND -filter:retweet\"\n",
        "\n",
        "        if filter_replies:\n",
        "          query += \" AND -filter:replies\"\n",
        "\n",
        "      else:\n",
        "        query='conversation_id:'+ conversation_id\n",
        "\n",
        "      if only_english:\n",
        "        query += ' lang:en'\n",
        "      \n",
        "      \n",
        "      #As df (from input) is just needed for the columns we set df to be equal to its first row\n",
        "      df = df.iloc[0:1]\n",
        "\n",
        "      #looping thorugh columns and clearing content in df\n",
        "      for col in df.columns.tolist():\n",
        "        if type(df[col].iloc[0]) == np.float64:\n",
        "          df[col] = [np.nan for _ in range(len(df))]\n",
        "        else:\n",
        "          df[col] = ['' for _ in range(len(df))]\n",
        "\n",
        "      #Specifying tweet specific variables\n",
        "      df['tweet_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_created_at'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_text'] = ['' for _ in range(len(df))]\n",
        "      df['length_of_text'] = [np.nan for _ in range(len(df))]\n",
        "      df['is_retweet'] = [np.nan  for _ in range(len(df))]\n",
        "      df['is_reply'] = [np.nan  for _ in range(len(df))]\n",
        "      df['tweet_mentions_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['tweet_mentions_user_screen_name'] = [np.nan for _ in range(len(df))]\n",
        "      df['quote_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['number_of_tweet_mentions'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweet_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['is_retweeted'] = [np.nan for _ in range(len(df))]\n",
        "      df['favorite_count'] = [np.nan for _ in range(len(df))]\n",
        "      df['contains_media'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_types'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_source_status_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_original_source_status_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_source_user_ids'] = [np.nan for _ in range(len(df))]\n",
        "      df['media_original_source_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['source'] = ['' for _ in range(len(df))]\n",
        "\n",
        "      #Too few entries to be used as features, but can be used for later network analysis\n",
        "      df['retweeted_status_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweeted_status_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['retweeted_status_user_screen_name'] = ['' for _ in range(len(df))]\n",
        "      df['in_reply_to_user_id'] = [np.nan for _ in range(len(df))]\n",
        "      df['in_reply_to_status_id'] = [np.nan for _ in range(len(df))]\n",
        "\n",
        "      #Setting column types that can hold lists for relevant columns\n",
        "      for col in ['tweet_mentions_user_id',\n",
        "                  'tweet_mentions_user_screen_name',\n",
        "                  'media_ids',\n",
        "                  'media_types',\n",
        "                  'media_source_status_ids',\n",
        "                  'media_source_user_ids',\n",
        "                  'media_original_source_status_id',\n",
        "                  'media_original_source_user_id']:\n",
        "        \n",
        "        df[col] = df[col].astype(object)\n",
        "\n",
        "\n",
        "      #Wrapping it in a try loop so features can be derived from what is collected in case there is an error\n",
        "      try:\n",
        "        #Defining collection method based on input\n",
        "        if search_tweets:\n",
        "          for entry in tqdm(tw.Cursor(api.search_tweets,\n",
        "                                        q=query,\n",
        "                                        since=date_since.strftime('%Y-%m-%d'),\n",
        "                                        until=date_until.strftime('%Y-%m-%d'),\n",
        "                                        result_type  = search_tweets_result_type,\n",
        "                                        tweet_mode=search_tweets_tweet_mode).items(search_tweets_total_queries)):\n",
        "            \n",
        "            #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "            entry_lst.append(entry)\n",
        "            \n",
        "            #Having the system sleep between each entry to limit the load on Twitter API\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        elif search_30_day:\n",
        "          for i in range(search_30_day_number_of_days):\n",
        "            \n",
        "            try:\n",
        "              \n",
        "              for entry in tqdm(api.search_30_day(query = query, \n",
        "                                    label = search_30_day_label,\n",
        "                                    fromDate = (date_since -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                    toDate = (date_until -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                    maxResults = search_30_day_maxResults\n",
        "                                    )):\n",
        "                \n",
        "                #Storring entry_lst at storing_intervall\n",
        "                if len(entry_lst) % storing_intervall == 0 and len(entry_lst) != 0:\n",
        "                  pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                  print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                      \n",
        "\n",
        "                #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "                entry_lst.append(entry)\n",
        "                \n",
        "                #Having the system sleep between each entry to limit the load on Twitter API\n",
        "                time.sleep(0.5)\n",
        "            \n",
        "            except:\n",
        "              pass\n",
        "\n",
        "        \n",
        "        elif search_full_archive:\n",
        "\n",
        "          for i in range(search_full_archive_number_of_days):\n",
        "\n",
        "            try:\n",
        "              \n",
        "              for entry in tqdm(api.search_full_archive(query = query, \n",
        "                                    label = search_full_archive_label,\n",
        "                                    fromDate = (date_since -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                    toDate = (date_until -timedelta(days = i)).strftime('%Y%m%d%H%m'),\n",
        "                                    maxResults = search_full_archive_maxResults\n",
        "                                    )):\n",
        "                \n",
        "                #Storring entry_lst at storing_intervall\n",
        "                if len(entry_lst) % storing_intervall == 0 and len(entry_lst) != 0:\n",
        "                  pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                  print(\"File stored at: \"+output_folder_path+\"/entry_lst_length_\"+str(len(entry_lst))+\".json\")\n",
        "                          \n",
        "                \n",
        "                #As cursor only allows for one look, we loop through it and add entries to entry_lst\n",
        "                entry_lst.append(entry)\n",
        "                \n",
        "                #Having the system sleep between each entry to limit the load on Twitter API\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except:\n",
        "              pass    \n",
        "      \n",
        "      except Exception as e:\n",
        "\n",
        "        #Sending oha.digi@cbs.dk an email if exception is triggered\n",
        "        #Getting local IP\n",
        "        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
        "        s.connect((\"8.8.8.8\", 80))\n",
        "        LOCAL_IP = s.getsockname()[0]\n",
        "        s.close()\n",
        "\n",
        "        #Setting up connection to my gmail\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as connection:  \n",
        "            email_address = 'ohauglend@gmail.com'\n",
        "            email_password = 'xoughywacvkfmkhi'\n",
        "            connection.login(email_address, email_password )\n",
        "            connection.sendmail(from_addr=email_address, to_addrs='oha.digi@cbs.dk', \n",
        "            msg=\"IP: \"+str(LOCAL_IP)+\"\\nextract_twitter_data Cursor failed, Error: \\t\"+ str(e) + '\\nNumber of entries in Cursor:\\t' +str(len(entry_lst))\n",
        "            )\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "      #Storing entry_lst in case something happens in populating df\n",
        "      pd.Series(entry_lst).to_json(output_folder_path+\"/entry_lst.json\")\n",
        "\n",
        "      #looping through entry_lst to populate df\n",
        "      for i in range(len(entry_lst)):\n",
        "\n",
        "        #Storing df in output_folder_path as a .json file with the user_id of the last user to have features populated\n",
        "        #Not storing on the first iteration of df\n",
        "        if i % storing_intervall == 0 and i != 0:\n",
        "            df.to_json(output_folder_path+\"/last_user_id_\"+str(df.user_id.iloc[i-1])+\".json\")\n",
        "            print(\"File stored at: \"+output_folder_path+\"/last_user_id_\"+str(df.user_id.iloc[i-1])+\".json\")\n",
        "        \n",
        "        #Adding user specific features to the df\n",
        "        #When storing df to json the date will be casted to the correct format\n",
        "        df.at[i, 'created_at_1'] = entry_lst[i].user.created_at\n",
        "        df.at[i, 'user_id'] = entry_lst[i].user.id\n",
        "        df.at[i, 'user_name'] = entry_lst[i].user.name\n",
        "        df.at[i, 'user_screen_name'] = entry_lst[i].user.screen_name\n",
        "        df.at[i,'user_location'] = entry_lst[i].user.location\n",
        "\n",
        "        #user_description \n",
        "        if type(entry_lst[i].user.description) == str:\n",
        "          df.at[i, 'user_description'] = entry_lst[i].user.description\n",
        "\n",
        "\n",
        "        #user_url \n",
        "        if type(entry_lst[i].user.url) == str:\n",
        "            df.at[i,'user_url'] = entry_lst[i].user.url\n",
        "\n",
        "        \n",
        "        #protected \n",
        "        if type(entry_lst[i].user.protected) == bool:\n",
        "            df.at[i,'protected'] = float(entry_lst[i].user.protected)\n",
        "\n",
        "\n",
        "        #followers_count \n",
        "        if type(entry_lst[i].user.followers_count) == int:\n",
        "          df.at[i,'followers_count'] = float(entry_lst[i].user.followers_count)\n",
        "\n",
        "        \n",
        "        #friends_count \n",
        "        if type(entry_lst[i].user.friends_count) == int:\n",
        "          df.at[i,'friends_count'] = float(entry_lst[i].user.friends_count)\n",
        "\n",
        "\n",
        "        #listed_count \n",
        "        if type(entry_lst[i].user.listed_count) == int:\n",
        "          df.at[i,'listed_count'] = float(entry_lst[i].user.listed_count)\n",
        "\n",
        "\n",
        "\n",
        "        #favourites_count \n",
        "        if type(entry_lst[i].user.favourites_count) == int:\n",
        "          df.at[i,'favourites_count'] = float(entry_lst[i].user.favourites_count)\n",
        "\n",
        "\n",
        "        #utc_offset \n",
        "        #missing values are marked as np.nan meaning comparing on type (float) does not work\n",
        "        if type(entry_lst[i].user.utc_offset) == str:\n",
        "          df.at[i,'utc_offset'] = entry_lst[i].user.utc_offset\n",
        "\n",
        "\n",
        "        #time_zone \n",
        "        if type(entry_lst[i].user.time_zone) == str:\n",
        "          df.at[i,'time_zone'] = entry_lst[i].user.time_zone\n",
        "\n",
        "\n",
        "        #geo_enabled \n",
        "        if type(entry_lst[i].user.geo_enabled) == bool:\n",
        "          df.at[i,'geo_enabled'] = float(entry_lst[i].user.geo_enabled)\n",
        "\n",
        "\n",
        "        #verified - might change - for now: all acounts except 3 in dataset have binary labels if verified or not :\n",
        "        if type(entry_lst[i].user.verified) == bool:\n",
        "          df.at[i,'verified'] = float(entry_lst[i].user.verified)\n",
        "\n",
        "          \n",
        "        #statuses_count -update if new\n",
        "        if type(entry_lst[i].user.statuses_count) == int:\n",
        "          df.at[i,'statuses_count'] = float(entry_lst[i].user.statuses_count)\n",
        "\n",
        "\n",
        "        #lang \n",
        "        if type(entry_lst[i].user.lang) == str:\n",
        "          df.at[i,'lang'] = entry_lst[i].user.lang\n",
        "\n",
        "\n",
        "        #contributors_enabled \n",
        "        if type(entry_lst[i].user.contributors_enabled) == bool:\n",
        "          df.at[i,'contributors_enabled'] = float(entry_lst[i].user.contributors_enabled)\n",
        "\n",
        "\n",
        "        #is_translator \n",
        "        if type(entry_lst[i].user.is_translator) == bool:\n",
        "          df.at[i,'is_translator'] = float(entry_lst[i].user.is_translator)\n",
        "\n",
        "\n",
        "        #profile_background_color \n",
        "        if type(entry_lst[i].user.profile_background_color) == str:\n",
        "          df.at[i,'profile_background_color'] = entry_lst[i].user.profile_background_color\n",
        "\n",
        "\n",
        "        #profile_background_image_url \n",
        "        if type(entry_lst[i].user.profile_background_image_url) == str:\n",
        "          df.at[i,'profile_background_image_url'] = entry_lst[i].user.profile_background_image_url\n",
        "\n",
        "\n",
        "        #profile_background_image_url_https \n",
        "        if type(entry_lst[i].user.profile_background_image_url_https) == str:\n",
        "          df.at[i,'profile_background_image_url_https'] = entry_lst[i].user.profile_background_image_url_https\n",
        "\n",
        "\n",
        "        #profile_background_tile \n",
        "        if type(entry_lst[i].user.profile_background_tile) == bool:\n",
        "          df.at[i,'profile_background_tile'] = float(entry_lst[i].user.profile_background_tile)\n",
        "\n",
        "\n",
        "        #profile_image_url \n",
        "        if type(entry_lst[i].user.profile_image_url) == str:\n",
        "          df.at[i,'profile_image_url'] = entry_lst[i].user.profile_image_url\n",
        "          \n",
        "\n",
        "        #profile_image_url_https \n",
        "        if type(entry_lst[i].user.profile_image_url_https) == str:\n",
        "          df.at[i,'profile_image_url_https'] = entry_lst[i].user.profile_image_url_https\n",
        "\n",
        "\n",
        "        #profile_link_color \n",
        "        if type(entry_lst[i].user.profile_link_color) == str:\n",
        "          df.at[i,'profile_link_color'] = entry_lst[i].user.profile_link_color\n",
        "\n",
        "\n",
        "        #profile_sidebar_border_color \n",
        "        if type(entry_lst[i].user.profile_sidebar_border_color) == str:\n",
        "          df.at[i,'profile_sidebar_border_color'] = entry_lst[i].user.profile_sidebar_border_color\n",
        "\n",
        "\n",
        "        #profile_sidebar_fill_color \n",
        "        if type(entry_lst[i].user.profile_sidebar_fill_color) == str:\n",
        "          df.at[i,'profile_sidebar_fill_color'] = entry_lst[i].user.profile_sidebar_fill_color\n",
        "\n",
        "\n",
        "        #profile_text_color \n",
        "        if type(entry_lst[i].user.profile_text_color) == str:\n",
        "          df.at[i,'profile_text_color'] = entry_lst[i].user.profile_text_color\n",
        "          \n",
        "\n",
        "        #profile_use_background_image \n",
        "        if type(entry_lst[i].user.profile_use_background_image) == bool:\n",
        "          df.at[i,'profile_use_background_image'] = float(entry_lst[i].user.profile_use_background_image)\n",
        "\n",
        "\n",
        "        #translator_type \n",
        "        if type(entry_lst[i].user.translator_type) == str:\n",
        "          df.at[i,'translator_type'] = entry_lst[i].user.translator_type\n",
        "\n",
        "\n",
        "        #withheld \n",
        "        if type(entry_lst[i].user.withheld_in_countries) == list:\n",
        "          df.at[i,'withheld'] = entry_lst[i].user.withheld_in_countries\n",
        "\n",
        "        #default_profile \n",
        "        if type(entry_lst[i].user.default_profile) == bool:\n",
        "          df.at[i,'default_profile'] = float(entry_lst[i].user.default_profile)\n",
        "\n",
        "        #default_profile_image \n",
        "        if type(entry_lst[i].user.default_profile_image) == bool:\n",
        "          df.at[i,'default_profile_image'] = float(entry_lst[i].user.default_profile_image)\n",
        "        \n",
        "\n",
        "\n",
        "        #Adding the Tweet specific features to df\n",
        "        df.at[i, 'tweet_id'] = entry_lst[i].id\n",
        "        df.at[i, 'tweet_created_at'] = entry_lst[i].created_at\n",
        "        \n",
        "        if search_tweets:\n",
        "          df.at[i, 'tweet_text'] = entry_lst[i].full_text\n",
        "          df.at[i, 'length_of_text'] = len(entry_lst[i].full_text)\n",
        "\n",
        "        else:\n",
        "          df.at[i, 'tweet_text'] = entry_lst[i].text\n",
        "          df.at[i, 'length_of_text'] = len(entry_lst[i].text)\n",
        "\n",
        "\n",
        "        \n",
        "        try:\n",
        "          entry_lst[i].retweeted_status\n",
        "          df.at[i, 'is_retweet'] = float(True)\n",
        "\n",
        "          #Although these are too few entries to be used as features, they can be useful for later network-based approaches\n",
        "          df.at[i, 'retweeted_status_id'] = entry_lst[i].retweeted_status.id\n",
        "          df.at[i, 'retweeted_status_user_id'] = entry_lst[i].retweeted_status.user.id\n",
        "          df.at[i, 'retweeted_status_user_screen_name'] = entry_lst[i].retweeted_status.user.screen_name\n",
        "\n",
        "\n",
        "        except AttributeError:\n",
        "          df.at[i, 'is_retweet'] = float(False)\n",
        "\n",
        "        try:\n",
        "          entry_lst[i].in_reply_to_status_id\n",
        "          df.at[i, 'is_reply'] = float(True)\n",
        "\n",
        "          #Although these are too few entries to be used as features, they can be useful for later network-based approaches\n",
        "          df.at[i, 'in_reply_to_status_id'] = entry_lst[i].in_reply_to_status_id\n",
        "          df.at[i, 'in_reply_to_user_id'] = entry_lst[i].in_reply_to_user_id\n",
        "\n",
        "\n",
        "        except:\n",
        "          df.at[i, 'is_reply'] = float(False)\n",
        "        \n",
        "        \n",
        "        df.at[i, 'tweet_mentions_user_id'] = [entry_lst[i].entities['user_mentions'][_]['id'] for _ in range(len(entry_lst[i].entities['user_mentions']))]\n",
        "        df.at[i, 'tweet_mentions_user_screen_name'] = [entry_lst[i].entities['user_mentions'][_]['screen_name'] for _ in range(len(entry_lst[i].entities['user_mentions']))]\n",
        "        df.at[i, 'number_of_tweet_mentions'] = len(entry_lst[i].entities['user_mentions'])\n",
        "\n",
        "        try:\n",
        "          df.at[i, 'quote_count'] = entry_lst[i].quote_count\n",
        "        except AttributeError:\n",
        "          df.at[i, 'quote_count'] = 0\n",
        "\n",
        "        \n",
        "        df.at[i, 'retweet_count'] = entry_lst[i].retweet_count\n",
        "        if (entry_lst[i].retweet_count == 0):\n",
        "          df.at[i, 'is_retweeted'] = float(True)\n",
        "        else:\n",
        "          df.at[i, 'is_retweeted'] = float(False)\n",
        "\n",
        "        \n",
        "        try:\n",
        "          df.at[i, 'favorite_count'] = entry_lst[i].favorite_count\n",
        "        except AttributeError:\n",
        "          df.at[i, 'favorite_count'] = 0\n",
        "\n",
        "        try:\n",
        "          entry_lst[i].extended_entities['media']\n",
        "          df.at[i, 'contains_media'] = float(True)\n",
        "          df.at[i, 'media_ids'] = [entry_lst[i].extended_entities['media'][_].get('id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "          df.at[i, 'media_types'] = [entry_lst[i].extended_entities['media'][_].get('type') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "\n",
        "          if [entry_lst[i].extended_entities['media'][_].get('source_status_id') for _ in range(len(entry_lst[i].extended_entities['media']))] != [None]:\n",
        "            df.at[i, 'media_source_status_ids'] = [entry_lst[i].extended_entities['media'][_].get('source_status_id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "            df.at[i, 'media_original_source_status_id'] = [entry_lst[i].extended_entities['media'][_].get('source_status_id') == entry_lst[i].id for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "\n",
        "          if [entry_lst[i].extended_entities['media'][_].get('source_user_id') for _ in range(len(entry_lst[i].extended_entities['media']))] != [None]:\n",
        "            df.at[i, 'media_source_user_ids'] = [entry_lst[i].extended_entities['media'][_].get('source_user_id') for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "            df.at[i, 'media_original_source_user_id'] = [entry_lst[i].extended_entities['media'][_].get('source_user_id') == entry_lst[i].user.id for _ in range(len(entry_lst[i].extended_entities['media']))]\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        except AttributeError:\n",
        "          df.at[i, 'contains_media'] = float(False)\n",
        "\n",
        "        \n",
        "        df.at[i,'source'] = entry_lst[i].source\n",
        "\n",
        "\n",
        "        #Specifying yang et al features\n",
        "        try:\n",
        "          #Getting user age as a variable to compute the ratios - also storing user age in the df but it might not be used as a feature\n",
        "          user_age = int(time.mktime(df.tweet_created_at.iloc[i].timetuple())) - int(time.mktime(df.created_at_1.iloc[i].timetuple()))\n",
        "          df.at[i, 'user_age'] = user_age\n",
        "          df.at[i, 'tweet_frequency'] = df.statuses_count.iloc[i]/user_age\n",
        "          df.at[i, 'followers_growth_rate'] = df.followers_count.iloc[i]/user_age\n",
        "          df.at[i, 'friends_growth_rate'] = df.friends_count.iloc[i]/user_age\n",
        "          df.at[i, 'favourites_growth_rate'] = df.favourites_count.iloc[i]/user_age\n",
        "          df.at[i, 'listed_growth_rate'] = df.listed_count.iloc[i]/user_age\n",
        "        \n",
        "          #Poulating features non-dependent on age\n",
        "          df.at[i, 'followers_friends_ratio'] = df.followers_count.iloc[i]/df.friends_count.iloc[i]\n",
        "          df.at[i, 'screen_name_length'] = len(df.user_screen_name.iloc[i])\n",
        "          df.at[i, 'num_digits_in_screen_name'] = sum(c.isdigit() for c in df.user_screen_name.iloc[i])\n",
        "          df.at[i, 'name_length'] = len(df.user_name.iloc[i])\n",
        "          df.at[i, 'num_digits_in_name'] = sum(c.isdigit() for c in df.user_name.iloc[i])\n",
        "          df.at[i, 'description_length'] = len(df.user_description.iloc[i])\n",
        "        \n",
        "        except TypeError as e:\n",
        "          print(\"Exception in Yang et al features, triggered by user:\\t\"+str(df.user_id.iloc[i])+\"\\nerror:\\t\"+str(e))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #Storing one final copy after exiting the for loop\n",
        "      df.to_json(output_folder_path+\"/FINAL_last_user_id_\"+str(df.user_id.iloc[i])+\".json\")\n",
        "      print(\"Twitter keyword search file stored at: \"+output_folder_path+\"/FINAL_last_user_id_\"+str(df.user_id.iloc[i])+\".json\")\n",
        "\n",
        "      data_path = output_folder_path+\"/FINAL_last_user_id_\"+str(df.user_id.iloc[i])+\".json\"\n",
        "\n",
        "\n",
        "      #Generating embeddings file based on RUN_ID, and specified output_folder_path, using path to FINAL_last_user_id file as input_df_path\n",
        "      if generate_embeddings_file:\n",
        "        embeddings_path = self.generate_embeddings(RUN_ID = RUN_ID,\n",
        "                                 input_df_path = output_folder_path+\"/FINAL_last_user_id_\"+str(df.user_id.iloc[i])+\".json\",\n",
        "                                 output_folder_path = output_folder_path,\n",
        "                                 local_repository = local_repository)\n",
        "        \n",
        "        return data_path, embeddings_path\n",
        "\n",
        "      \n",
        "      return data_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def generate_embeddings(self,\n",
        "                            RUN_ID,\n",
        "                            input_df_path,\n",
        "                            output_folder_path,\n",
        "                            local_repository = '/content/'):\n",
        "       \n",
        "        \"\"\"\n",
        "        The function generates embeddings from textual features and returns the filepath where this is stored. \n",
        "        The function extracts the features and the embeddign model used from the model's associated RUN_ID\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        #Making a copy of the df from input_df_path\n",
        "        df = pd.read_json(input_df_path)    \n",
        "        tokenized_df = copy.deepcopy(df)\n",
        "\n",
        "      \n",
        "        #loading in feature_lst as the model_feature_lsit.txt artifact assosiated with the passed in RUN_ID\n",
        "        feature_lst = ast.literal_eval(mlflow.artifacts.load_text('runs:/'+ RUN_ID +'/model_feature_list.txt'))\n",
        "        tokenized_df[tokenized_df.columns[~tokenized_df.columns.isin(feature_lst)]]\n",
        "\n",
        "        #isolating numeric columns\n",
        "        number_columns = df._get_numeric_data().columns.tolist()\n",
        "        number_columns.extend(['tweet_mentions_user_id', 'tweet_mentions_user_screen_name', 'media_ids', 'media_source_status_ids', 'media_original_source_status_id', 'media_source_user_ids', 'media_original_source_user_id'])\n",
        "\n",
        "        #Isolating text columns as columns not in number_columns\n",
        "        text_columns = []\n",
        "        for col in tokenized_df.columns.tolist():\n",
        "          if col not in number_columns:\n",
        "            text_columns.append(col)\n",
        "\n",
        "        #Downloading tokenizer as the tokenizer artifact assosiated with the passed in RUN_ID\n",
        "        #Storing artifact in local repository for colab this is '/content/'\n",
        "        mlflow.artifacts.download_artifacts('runs:/'+ RUN_ID +'/tokenizer', dst_path = local_repository)\n",
        "        \n",
        "        tokenizer = AutoTokenizer.from_pretrained(local_repository+'tokenizer')\n",
        "\n",
        "\n",
        "        #Performing tokenization on text columns - extracting input_ids\n",
        "        for col in tqdm(text_columns):\n",
        "\n",
        "          #Encoding missing values as an empty string  \n",
        "          tokenized_df[col] = tokenized_df[col].map(lambda x: x if type(x) == str else '')\n",
        "\n",
        "          #extracting and storing input_ids\n",
        "          tokenized_df[col] = tokenized_df[col].map(lambda x: tokenizer(x)['input_ids'])\n",
        "        \n",
        "\n",
        "        #Outputting the tokenized_df to the specified output_folder_path\n",
        "        tokenized_df.to_json(output_folder_path+'/tokenized_data_last_user_id_'+str(tokenized_df.user_id.iloc[-1])+'.json')\n",
        "        \n",
        "        print('Embeddings file stored at: '+output_folder_path+'/tokenized_data_last_user_id_'+str(tokenized_df.user_id.iloc[-1])+'.json')\n",
        "\n",
        "        return output_folder_path+'/tokenized_data_last_user_id_'+str(tokenized_df.user_id.iloc[-1])+'.json'\n",
        "\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def isolate_humans_and_bots(self,\n",
        "                                embeddings_path,\n",
        "                                data_path,\n",
        "                                dataset_cut_off = 5):\n",
        "      \n",
        "      \"\"\"\n",
        "      The function uses self to identify likely humans and likely bots from domain data as the dataset_cut_off percentage of the most extreme bot score.\n",
        "      The intention is to use model functionality such as extract_twitter_data_keyword to obtain new domain data, which should be then fed into this function.\n",
        "      The user needs to specify paths to embedded data (for prediction) and raw textual data (for isolation/selectiion)\n",
        "      dataset_cut_off is defaulted to 5 meaning 5% of the accoutns will be labeled as likely_humans, and 5% as likely_bots\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "      embeddings_data = pd.read_json(embeddings_path)\n",
        "      data_untokenized = pd.read_json(data_path)\n",
        "\n",
        "      #Need to create a dummy and populate 2 features with dummy bolean\n",
        "      embeddings_data['Bot'] = 1\n",
        "      embeddings_data = embeddings_data.drop(['withheld'], axis = 1)\n",
        "      embeddings_data['is_translation_enabled'] = 0\n",
        "      embeddings_data['has_extended_profile'] = 0\n",
        "\n",
        "      #processing step needed on untokenized data\n",
        "      data_untokenized['tweet_created_at'].apply(lambda x: np.nan if type(x) == pd._libs.tslibs.nattype.NaTType else int(time.mktime(x.timetuple())))\n",
        "\n",
        "      predict_ds = generate_batch_ds(df = embeddings_data,\n",
        "                                  embeddings_encoder_type = tf.int64)\n",
        "\n",
        "      pred = self.predict(predict_ds)\n",
        "\n",
        "      bot_tweets = embeddings_data[np.where(np.searchsorted(np.percentile(pred, [100 - dataset_cut_off]), pred) == 1, 1, 100) == [1]][['user_id','tweet_id']]\n",
        "      human_tweets = embeddings_data[np.where(np.searchsorted(np.percentile(pred, [dataset_cut_off]), pred) == 1, 1, 100) == [1]][['user_id','tweet_id']]\n",
        "\n",
        "      likely_bots_df = data_untokenized[data_untokenized['tweet_id'].isin(bot_tweets['tweet_id'].tolist())].drop_duplicates('tweet_id').reset_index(drop = True)\n",
        "      likely_humans_df = data_untokenized[data_untokenized['tweet_id'].isin(human_tweets['tweet_id'].tolist())].drop_duplicates('tweet_id').reset_index(drop = True)\n",
        "\n",
        "      return likely_bots_df, likely_humans_df\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def numerical_and_boolean_features(self,\n",
        "                                      RUN_ID,\n",
        "                                      likely_bots_df,\n",
        "                                      likely_humans_df\n",
        "                                      local_repository ='/content'):\n",
        "      \"\"\"\n",
        "      The function analyses likely_humans and likely_bots for mean values and statistical differences in their features.\n",
        "      RUN_ID is used to extract features from the databricks server associated with the model.\n",
        "      More specifically, the student-t test and the ANOVA test are employed to check if the numerical and Boolean features are drawn from the same distribution and have the same population mean.\n",
        "      The analysis result as a nested dictionary following the schema: {feature_name: {Bot: {min: val, max: val, mean: val}}, {Human: {min: val, max: val, mean: val}}, \n",
        "      #{Difference: {mean: val, t-stat: (statistic, pvalue), ANOVA: (statistic, pvalue)} is stored in the local_repository.\n",
        "\n",
        "      The function has no return\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      #Extracting the features associated with the RUN_ID from databricks\n",
        "      feature_lst = ast.literal_eval(mlflow.artifacts.load_text('runs:/'+ RUN_ID +'/model_feature_list.txt'))\n",
        "\n",
        "      #Isolating numerical and boolean features\n",
        "      num_or_boolean_features = list(set(likely_bots_df.select_dtypes(include=np.number).columns.tolist()) & set(feature_lst))\n",
        "\n",
        "      #Isolating numerical and boolean features\n",
        "      num_features = []\n",
        "      boolean_features = []\n",
        "\n",
        "      #Looping through num_or_boolean_features, appending col to boolean_features if the only values in likely_humans_df or likely_humans_df is 0 and 1\n",
        "      for col in num_or_boolean_features:\n",
        "\n",
        "        #Excluding features that only have one or fewer possible values\n",
        "        if len(likely_humans_df[col].value_counts().index.tolist()) <= 1 and len(likely_bots_df[col].value_counts().index.tolist()) <= 1 :\n",
        "          continue\n",
        "\n",
        "        #Two possible values in the features means that it will be treated as a boolean feature\n",
        "        if len(likely_humans_df[col].dropna().value_counts().index.tolist()) <= 2 or len(likely_bots_df[col].dropna().value_counts().index.tolist()) <= 2:\n",
        "          boolean_features.append(col)\n",
        "        else:\n",
        "          num_features.append(col)\n",
        "\n",
        "      #Syntax: {feature_name: {Bot: {min: val, max: val, mean: val}}, {Human: {min: val, max: val, mean: val}}, \n",
        "      #{Difference: {mean: val, t-stat: (statistic, pvalue), ANOVA: (statistic, pvalue)}\n",
        "      num_features_dic = {}\n",
        "      for feature in num_features:\n",
        "        \n",
        "        feature_dic ={}\n",
        "\n",
        "\n",
        "        bot_feature_series = likely_bots_df[feature].dropna()\n",
        "        feature_dic['Bot'] = {'min': float(np.min(bot_feature_series)), \n",
        "                              'max': float(np.max(bot_feature_series)),\n",
        "                              'mean': float(np.mean(bot_feature_series))\n",
        "                              }\n",
        "        \n",
        "        human_feature_series = likely_humans_df[feature].dropna()\n",
        "        feature_dic['Human'] = {'min': float(np.min(human_feature_series)), \n",
        "                              'max': float(np.max(human_feature_series)),\n",
        "                              'mean': float(np.mean(human_feature_series))\n",
        "                              }\n",
        "\n",
        "        \n",
        "        feature_dic['Difference'] = {'mean': float(np.mean(bot_feature_series)) - float(np.mean(human_feature_series)), \n",
        "                              't-stat': st.ttest_ind(bot_feature_series.array, human_feature_series.array, equal_var = True),\n",
        "                              'ANOVA': st.f_oneway(bot_feature_series.array, human_feature_series.array)\n",
        "                              }\n",
        "        \n",
        "        num_features_dic[feature] = feature_dic\n",
        "\n",
        "\n",
        "      #Syntax: {feature_name: {Bot: {mean: val}}, {Human: {mean: val}}, \n",
        "      #{Difference: {mean: val, t-stat: (statistic, pvalue), ANOVA: (statistic, pvalue)}\n",
        "      boolean_features_dic = {}\n",
        "      for feature in boolean_features:\n",
        "        \n",
        "        feature_dic ={}\n",
        "\n",
        "\n",
        "        bot_feature_series = likely_bots_df[feature].dropna()\n",
        "        feature_dic['Bot'] = {'mean': float(np.mean(bot_feature_series))\n",
        "                              }\n",
        "        \n",
        "        human_feature_series = likely_humans_df[feature].dropna()\n",
        "        feature_dic['Human'] = {'mean': float(np.mean(human_feature_series))\n",
        "                              }\n",
        "\n",
        "        \n",
        "        feature_dic['Difference'] = {'mean': float(np.mean(bot_feature_series)) - float(np.mean(human_feature_series)), \n",
        "                              't-stat': st.ttest_ind(bot_feature_series.array, human_feature_series.array, equal_var = True),\n",
        "                              'ANOVA': st.f_oneway(bot_feature_series.array, human_feature_series.array)\n",
        "                              }\n",
        "        \n",
        "        boolean_features_dic[feature] = feature_dic\n",
        "\n",
        "      #Storing the feature dir in the local directory\n",
        "      feature_dic = {'num_features': num_features_dic, 'boolean_features': boolean_features_dic}\n",
        "\n",
        "      with open(local_repository+'/feature_dic', 'w') as fp:\n",
        "          json.dump(feature_dic, fp)\n",
        "\n",
        "    @classmethod\n",
        "    def frequent_hashtags(likely_bots_or_humans,\n",
        "                          local_repository ='/content'\n",
        "                          ):\n",
        "      \n",
        "      \"\"\"\n",
        "      The function analyses frequent hashtags of either likely_bots or likely_humans. \n",
        "        The user should input the whole data of either.\n",
        "      The results are stored in local_repository\n",
        "      The function has no return\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      hash_tag_series = likely_bots_or_humans.tweet_text.progress_apply(lambda x: re.findall(r'(#\\w+)', x))\n",
        "      sorted_hash_tag_series = hash_tag_series.progress_apply(lambda x: pd.Series(x)).stack().value_counts()\n",
        "\n",
        "      plt.style.use(plt.style.available[11])\n",
        "\n",
        "      hash_tags = hash_tag_series.apply(lambda x: pd.Series(x)).stack().value_counts().iloc[0:10].index.tolist()\n",
        "      values = hash_tag_series.apply(lambda x: pd.Series(x)).stack().value_counts().iloc[0:10].values\n",
        "\n",
        "      fig, ax = plt.subplots(figsize=(8,6))\n",
        "      plt.bar(hash_tags, \n",
        "              values, align='center')\n",
        "      plt.xticks(hash_tags)\n",
        "      plt.ylim(np.min(values)* 0.8, np.max(values)* 1.1)\n",
        "      plt.title('Frequency distribution of Hahstags',fontsize = 15)\n",
        "      ax.set_xticklabels(hash_tags, rotation = 45, ha=\"right\", fontsize=13)\n",
        "      ax.set_ylabel('Value')\n",
        "      ax.yaxis.label.set_fontsize(13)\n",
        "      plt.show()\n",
        "\n",
        "      plt.savefig(local_repository+'frequency_distribution_of_hashtags.png')\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tweet_text_processing(self,\n",
        "                              likely_bots_or_humans,\n",
        "                              local_repository ='/content' \n",
        "                              ):\n",
        "      \"\"\"\n",
        "      A helping function needed for processing tweet text.\n",
        "      The function returns processed tweet text from either likely_bots or likely_humans (the user needs to specify)\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "      #Removing URLs Mentions Reserved words (RT, FAV) Emojis Smileys\n",
        "      clean_tweet_series = likely_bots_or_humans.tweet_text.apply(lambda x: p.clean(x))\n",
        "\n",
        "      #removing digits\n",
        "      clean_tweet_series = clean_tweet_series.apply(lambda x: x.replace('\\d+', '').lower())\n",
        "\n",
        "      #removing punctuations\n",
        "      clean_tweet_series = pd.Series([re.sub(r'[^\\w\\s]', '', word) for word in clean_tweet_series if re.sub(r'[^\\w\\s]', '', word)])\n",
        "\n",
        "      #Lemmatizing \n",
        "      lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "      tokenizer = TweetTokenizer()\n",
        "\n",
        "      clean_tweet_series = clean_tweet_series.apply(lambda x: [(lemmatizer.lemmatize(w)) for w in tokenizer.tokenize((x))])\n",
        "\n",
        "      #removing stop-words\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      clean_tweet_series = clean_tweet_series.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "\n",
        "\n",
        "      return clean_tweet_series\n",
        "\n",
        "\n",
        "\n",
        "    def frequent_words(self,\n",
        "                      clean_tweet_series, \n",
        "                      local_repository ='/content'):\n",
        "      \n",
        "      \"\"\"\n",
        "      The function analyses the frequent words of the clean_tweet_series inputed for frequent words\n",
        "      The function stores a bar plot in local_repository and has no return \n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "      words = clean_tweet_series.apply(lambda x: pd.Series(x)).stack().value_counts().iloc[0:10].index\n",
        "      values = clean_tweet_series.apply(lambda x: pd.Series(x)).stack().value_counts().iloc[0:10].values\n",
        "\n",
        "      plt.style.use(plt.style.available[11])\n",
        "      fig, ax = plt.subplots(figsize=(12,8))\n",
        "      plt.bar(words, \n",
        "              values, align='center')\n",
        "      \n",
        "      plt.xticks(words)\n",
        "      plt.ylim(0, np.max(values)* 1.1)\n",
        "      plt.title('Frequency distribution of words in bot Tweets',fontsize = 24)\n",
        "      ax.set_xticklabels(words, rotation = 45, ha=\"right\", fontsize=20)\n",
        "      ax.set_ylabel('Value')\n",
        "      ax.yaxis.set_tick_params(labelsize=20)\n",
        "      ax.yaxis.label.set_fontsize(20)\n",
        "      plt.show()\n",
        "\n",
        "      plt.savefig(local_repository+'frequency_distribution_of_words.png')\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def topic_modelling(self,\n",
        "                        clean_tweet_series,\n",
        "                        local_repository ='/content',\n",
        "                        number_of_topics = [2,4,6,8,10],\n",
        "                        engine = \"text-davinci-003\",\n",
        "                        temperature = 0.7,\n",
        "                        max_tokens = 256,\n",
        "                        top_p = 1,\n",
        "                        frequency_penalty= 0,\n",
        "                        presence_penalty = 0,\n",
        "                        best_of = 3,\n",
        "                        openai_key =\"OPENAI_KEY\"):\n",
        "\n",
        "      \"\"\"\n",
        "      The function performs topic modelling on the inputed clean_tweet_series.\n",
        "      Using Latent Dirichlet Allocation (LDA), the model framework performs topic modelling to represent Tweets as a combination of latent topics \n",
        "      and each of these topics as combinations of different words.\n",
        "\n",
        "      The model first identifies a corpus in clean_tweet_series before it trains LDA models on different number of topics.\n",
        "        The number of topics are customizable by the user, but is defaulted to [2,4,6,8,10] to ensure interpretability and sufficient sample size.\n",
        "      The optimal number of topics is chosen by the model as the average of an elbow analysis of the perplexity curve and a knee analysis of the topic coherence scores\n",
        "      of the different topics.\n",
        "\n",
        "      Another LDA model with the optimal number of topics is then trained, its output as the word vector for every topic is then fed to a GPT3 model for topic labelling.\n",
        "        the egine is defaulted to \"text-davinci-003\" and the user needs to provide credentials through the openai_key parameter.\n",
        "        the parameters temperature, max_tokens, top_p, frequency_penalty, presence_penalty, and best_of are OpenAI api controlable parameters that is customizable by the user.\n",
        "\n",
        "      The model stores the topic labels and word-vectors in local_repository, before it creates and stores a pyLDAvis object as an html file\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      \n",
        "      openai.api_key = openai_key\n",
        "\n",
        "      # Build the bigram and trigram models\n",
        "      bigram = gensim.models.Phrases(clean_tweet_series, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "      trigram = gensim.models.Phrases(bigram[clean_tweet_series], threshold=100)\n",
        "\n",
        "\n",
        "      # Create Dictionary \n",
        "      id2word = corpora.Dictionary(clean_tweet_series)  \n",
        "      \n",
        "      # Term Document Frequency \n",
        "      corpus = [id2word.doc2bow(text) for text in clean_tweet_series]  \n",
        "\n",
        "      perplexity_log = {}\n",
        "      coherence_score_log = {}\n",
        "\n",
        "\n",
        "      for num_topics in number_of_topics:\n",
        "\n",
        "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                                id2word=id2word,\n",
        "                                                num_topics=num_topics, \n",
        "                                                random_state=100,\n",
        "                                                update_every=1,\n",
        "                                                chunksize=100,\n",
        "                                                passes=10,\n",
        "                                                alpha='auto',\n",
        "                                                per_word_topics=True)\n",
        "        \n",
        "\n",
        "        # Compute Perplexity - a measure of how good the model is. lower the better.\n",
        "        perplexity_log[str(num_topics)] = lda_model.log_perplexity(corpus)\n",
        "        \n",
        "        # Compute Coherence Score - a measure of how good the model is. higher the better.\n",
        "        coherence_score_log[str(num_topics)] = CoherenceModel(model=lda_model, \n",
        "                                                              texts=clean_tweet_series, \n",
        "                                                              dictionary=id2word, \n",
        "                                                              coherence='c_v').get_coherence()\n",
        "\n",
        "\n",
        "      plt.figure(figsize=(10, 7))\n",
        "\n",
        "\n",
        "      sns.lineplot(data=perplexity_log, color=\"g\", label=\"Perplexity\")\n",
        "      plt.legend(loc=4)\n",
        "      ax2 = plt.twinx()\n",
        "      sns.lineplot(data=coherence_score_log, color=\"b\", label=\"Coherence Score\", ax=ax2)\n",
        "\n",
        "      plt.title(\"Perplexity and Coherence Scores over number of topics\", fontsize=18)\n",
        "      plt.xlabel(\"Timestep\", fontsize=14)\n",
        "      plt.ylabel(\"Score\", fontsize=14)\n",
        "      plt.legend(loc=1)\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "      plt.savefig(local_repository+'knee_bow_analysis.png')\n",
        "\n",
        "      #Identifying the number of topics to be modelled using kneebow library\n",
        "      #Identifying the elbow in the perplexity_log curve and the knee in the coherence_score_log\n",
        "      #If conflicted the mean as an integer of the two values are chosen as the number of topics\n",
        "\n",
        "      #Defining a model to fit the series to\n",
        "      rotor = Rotor()\n",
        "\n",
        "      #identifying the elbow in perplexity_log\n",
        "      rotor.fit_rotate(np.array([[int(key), perplexity_log[key]] for key in perplexity_log.keys()]))\n",
        "      number_of_topics_perplexity_log = int(list(perplexity_log.keys())[rotor.get_elbow_index()])\n",
        "\n",
        "\n",
        "      #identifying the knee in coherence_score_log\n",
        "      rotor.fit_rotate(np.array([[int(key), coherence_score_log[key]] for key in coherence_score_log.keys()]))\n",
        "      number_of_topics_coherence_score_log = int(list(coherence_score_log.keys())[rotor.get_knee_index()])\n",
        "      #optional plot of elbow\n",
        "      #rotor.plot_knee()\n",
        "\n",
        "      num_topics = int(np.mean([number_of_topics_perplexity_log, number_of_topics_coherence_score_log]))\n",
        "\n",
        "      lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                                id2word=id2word,\n",
        "                                                num_topics=num_topics, \n",
        "                                                random_state=100,\n",
        "                                                update_every=1,\n",
        "                                                chunksize=100,\n",
        "                                                passes=10,\n",
        "                                                alpha='auto',\n",
        "                                                per_word_topics=True)\n",
        "\n",
        "\n",
        "      #Creating a list of topics and the words they compose of\n",
        "      lst = [(lda_model.print_topics()[_][0] + 1, lda_model.print_topics()[_][1]) for _ in range(len(lda_model.print_topics()))]\n",
        "\n",
        "\n",
        "      #Adding the task to the promt\n",
        "      corp = str([(lda_model.print_topics()[_][0] + 1, lda_model.print_topics()[_][1]) for _ in range(len(lda_model.print_topics()))])\n",
        "      corp += \"\\n\\nWhat are the \"+str(num_topics)+\" topics about output it as a list in the same order as the input?\"\n",
        "\n",
        "      \n",
        "          \n",
        "      #Generating string as the returned tweet from GPT-3\n",
        "      string =  openai.Completion.create(engine= engine,\n",
        "                                          prompt= corp, \n",
        "                                          temperature= temperature,\n",
        "                                          max_tokens= max_tokens,\n",
        "                                          top_p= top_p, \n",
        "                                          frequency_penalty= frequency_penalty,\n",
        "                                          presence_penalty=presence_penalty,\n",
        "                                          best_of = best_of)[\"choices\"][0].text.strip()\n",
        "\n",
        "      print('Labeled topics')\n",
        "      for s in string.split('\\n'):\n",
        "        print(s)\n",
        "\n",
        "      with open(local_repository+'Topic_labels.txt', 'w') as f:\n",
        "          for s in string.split('\\n'):\n",
        "            f.write(s)\n",
        "\n",
        "      # Print the keyword of topics\n",
        "      print('\\nTopic keywords:')\n",
        "      pprint([(lda_model.print_topics()[_][0] + 1, lda_model.print_topics()[_][1]) for _ in range(len(lda_model.print_topics()))])\n",
        "      doc_lda = lda_model[corpus]\n",
        "\n",
        "      with open(local_repository+'Topic_keywords.txt', 'w') as f:\n",
        "          f.write(str([(lda_model.print_topics()[_][0] + 1, lda_model.print_topics()[_][1]) for _ in range(len(lda_model.print_topics()))]))\n",
        "\n",
        "      print('\\npylda visualisation of topics')\n",
        "      #Note that the topic numbering is not the same as the previous schema\n",
        "      pyLDAvis.enable_notebook()\n",
        "      vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "\n",
        "      #saving as html file\n",
        "      pyLDAvis.save_html(vis, 'lda.html')\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing - making the data ready for the model"
      ],
      "metadata": {
        "id": "24gHZhQz1r-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection and batch mapping function"
      ],
      "metadata": {
        "id": "TGH-eZKU1r-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2[[\"Bot\",\n",
        "      \"user_location\",\n",
        "      \"user_description\",\n",
        "      \"protected\",\n",
        "      \"followers_count\",\n",
        "      \"friends_count\",\n",
        "      \"listed_count\",\n",
        "      \"favourites_count\",\n",
        "      \"geo_enabled\",\n",
        "      \"statuses_count\",\n",
        "      \"contributors_enabled\",\n",
        "      \"is_translator\",\n",
        "      \"is_translation_enabled\",\n",
        "      \"profile_background_color\",\n",
        "      \"profile_background_tile\",\n",
        "      \"profile_link_color\",\n",
        "      \"profile_sidebar_border_color\",\n",
        "      \"profile_sidebar_fill_color\",\n",
        "      \"profile_text_color\",\n",
        "      \"profile_use_background_image\",\n",
        "      \"translator_type\",\n",
        "      \"has_extended_profile\",\n",
        "      \"default_profile\",\n",
        "      \"default_profile_image\",\n",
        "      \"tweet_frequency\",\n",
        "      \"followers_growth_rate\",\n",
        "      \"friends_growth_rate\",\n",
        "      \"favourites_growth_rate\",\n",
        "      \"listed_growth_rate\",\n",
        "      \"followers_friends_ratio\",\n",
        "      \"screen_name_length\",\n",
        "      \"num_digits_in_screen_name\",\n",
        "      \"name_length\",\n",
        "      \"num_digits_in_name\",\n",
        "      \"description_length\",\n",
        "      \"media_types\",\n",
        "      \"tweet_text\",\n",
        "      \"length_of_text\",\n",
        "      \"is_retweet\",\n",
        "      \"is_reply\",\n",
        "      \"number_of_tweet_mentions\",\n",
        "      \"quote_count\",\n",
        "      \"retweet_count\",\n",
        "      \"is_retweeted\",\n",
        "      \"favorite_count\",\n",
        "      \"contains_media\",\n",
        "      \"source\"]]\n",
        "\n",
        "\n",
        "\n",
        "df3 = df3[[\"Bot\",\n",
        "      \"user_location\",\n",
        "      \"user_description\",\n",
        "      \"protected\",\n",
        "      \"followers_count\",\n",
        "      \"friends_count\",\n",
        "      \"listed_count\",\n",
        "      \"favourites_count\",\n",
        "      \"geo_enabled\",\n",
        "      \"statuses_count\",\n",
        "      \"contributors_enabled\",\n",
        "      \"is_translator\",\n",
        "      \"is_translation_enabled\",\n",
        "      \"profile_background_color\",\n",
        "      \"profile_background_tile\",\n",
        "      \"profile_link_color\",\n",
        "      \"profile_sidebar_border_color\",\n",
        "      \"profile_sidebar_fill_color\",\n",
        "      \"profile_text_color\",\n",
        "      \"profile_use_background_image\",\n",
        "      \"translator_type\",\n",
        "      \"has_extended_profile\",\n",
        "      \"default_profile\",\n",
        "      \"default_profile_image\",\n",
        "      \"tweet_frequency\",\n",
        "      \"followers_growth_rate\",\n",
        "      \"friends_growth_rate\",\n",
        "      \"favourites_growth_rate\",\n",
        "      \"listed_growth_rate\",\n",
        "      \"followers_friends_ratio\",\n",
        "      \"screen_name_length\",\n",
        "      \"num_digits_in_screen_name\",\n",
        "      \"name_length\",\n",
        "      \"num_digits_in_name\",\n",
        "      \"description_length\",\n",
        "      \"media_types\",\n",
        "      \"tweet_text\",\n",
        "      \"length_of_text\",\n",
        "      \"is_retweet\",\n",
        "      \"is_reply\",\n",
        "      \"number_of_tweet_mentions\",\n",
        "      \"quote_count\",\n",
        "      \"retweet_count\",\n",
        "      \"is_retweeted\",\n",
        "      \"favorite_count\",\n",
        "      \"contains_media\",\n",
        "      \"source\"]]\n",
        "\n",
        "\n",
        "\n",
        "df3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JziZ7D1AFEeH",
        "outputId": "0aaca8db-4c1a-4833-d6d3-881e49aecd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Bot                              user_location  \\\n",
              "0     0                                 [101, 102]   \n",
              "1     0        [101, 26947, 4360, 1010, 5978, 102]   \n",
              "2     0  [101, 8191, 14129, 2148, 1010, 2634, 102]   \n",
              "3     0    [101, 3693, 1996, 3524, 9863, 100, 102]   \n",
              "4     0                          [101, 16938, 102]   \n",
              "..  ...                                        ...   \n",
              "95    0                                 [101, 102]   \n",
              "96    0         [101, 2267, 2380, 1010, 9108, 102]   \n",
              "97    0                                 [101, 102]   \n",
              "98    1               [101, 2002, 1013, 2032, 102]   \n",
              "99    0                           [101, 2710, 102]   \n",
              "\n",
              "                                     user_description  protected  \\\n",
              "0   [101, 3243, 3497, 1996, 2087, 14953, 15485, 23...          0   \n",
              "1   [101, 100, 11924, 6958, 3992, 16770, 1024, 101...          0   \n",
              "2                                          [101, 102]          0   \n",
              "3   [101, 1037, 2451, 1997, 9683, 1010, 2199, 1988...          0   \n",
              "4                                          [101, 102]          0   \n",
              "..                                                ...        ...   \n",
              "95  [101, 15536, 14416, 21541, 24532, 2102, 1064, ...          0   \n",
              "96  [101, 11268, 1999, 1996, 29466, 1997, 8827, 17...          0   \n",
              "97  [101, 2023, 2003, 1996, 10474, 4070, 1997, 199...          0   \n",
              "98  [101, 2280, 6951, 2005, 5325, 3793, 2240, 1012...          0   \n",
              "99  [101, 1056, 28394, 2102, 2055, 1001, 4105, 100...          0   \n",
              "\n",
              "    followers_count  friends_count  listed_count  favourites_count  \\\n",
              "0              3473            753             8             33270   \n",
              "1              1578            382            18              1953   \n",
              "2               210            263             2             14689   \n",
              "3             85172              3           202               173   \n",
              "4                58            243             0             16659   \n",
              "..              ...            ...           ...               ...   \n",
              "95              291           1188             0               686   \n",
              "96              896            837             1              3193   \n",
              "97             5403           5464            57             21116   \n",
              "98              341           1171             1              2083   \n",
              "99             7602           7231           226             11305   \n",
              "\n",
              "    geo_enabled  statuses_count  ...  length_of_text  is_retweet  is_reply  \\\n",
              "0             0            4674  ...             104           0         1   \n",
              "1             0            1159  ...             291           0         1   \n",
              "2             0            1107  ...             140           1         1   \n",
              "3             0            1096  ...              32           0         1   \n",
              "4             1            4946  ...             140           1         1   \n",
              "..          ...             ...  ...             ...         ...       ...   \n",
              "95            1             330  ...             140           1         1   \n",
              "96            1             717  ...             144           1         1   \n",
              "97            1            8101  ...             140           1         1   \n",
              "98            0            1854  ...             144           1         1   \n",
              "99            0            8788  ...              75           0         1   \n",
              "\n",
              "   number_of_tweet_mentions  quote_count retweet_count is_retweeted  \\\n",
              "0                         1            0             0            0   \n",
              "1                         3            0             0            0   \n",
              "2                         1            0           737            1   \n",
              "3                         1            0             0            0   \n",
              "4                         2            0          1391            1   \n",
              "..                      ...          ...           ...          ...   \n",
              "95                        1            0             1            1   \n",
              "96                        2            0            55            1   \n",
              "97                        1            0            12            1   \n",
              "98                        1            0         10454            1   \n",
              "99                        0            0             0            0   \n",
              "\n",
              "   favorite_count contains_media                                     source  \n",
              "0              14              0             [101, 10474, 2005, 25249, 102]  \n",
              "1               0              0             [101, 10474, 2005, 18059, 102]  \n",
              "2               0              0             [101, 10474, 2005, 11924, 102]  \n",
              "3               3              0             [101, 10474, 4773, 10439, 102]  \n",
              "4               0              0             [101, 10474, 2005, 11924, 102]  \n",
              "..            ...            ...                                        ...  \n",
              "95              0              0             [101, 10474, 2005, 11924, 102]  \n",
              "96              0              0             [101, 10474, 2005, 18059, 102]  \n",
              "97              0              0             [101, 10474, 2005, 18059, 102]  \n",
              "98              0              0             [101, 10474, 4773, 10439, 102]  \n",
              "99              0              0  [101, 1056, 28394, 2102, 2214, 2695, 102]  \n",
              "\n",
              "[100 rows x 47 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20981972-6dcf-4848-9be8-abfcad0182ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bot</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>protected</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>friends_count</th>\n",
              "      <th>listed_count</th>\n",
              "      <th>favourites_count</th>\n",
              "      <th>geo_enabled</th>\n",
              "      <th>statuses_count</th>\n",
              "      <th>...</th>\n",
              "      <th>length_of_text</th>\n",
              "      <th>is_retweet</th>\n",
              "      <th>is_reply</th>\n",
              "      <th>number_of_tweet_mentions</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>is_retweeted</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>contains_media</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 102]</td>\n",
              "      <td>[101, 3243, 3497, 1996, 2087, 14953, 15485, 23...</td>\n",
              "      <td>0</td>\n",
              "      <td>3473</td>\n",
              "      <td>753</td>\n",
              "      <td>8</td>\n",
              "      <td>33270</td>\n",
              "      <td>0</td>\n",
              "      <td>4674</td>\n",
              "      <td>...</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 25249, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 26947, 4360, 1010, 5978, 102]</td>\n",
              "      <td>[101, 100, 11924, 6958, 3992, 16770, 1024, 101...</td>\n",
              "      <td>0</td>\n",
              "      <td>1578</td>\n",
              "      <td>382</td>\n",
              "      <td>18</td>\n",
              "      <td>1953</td>\n",
              "      <td>0</td>\n",
              "      <td>1159</td>\n",
              "      <td>...</td>\n",
              "      <td>291</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 18059, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 8191, 14129, 2148, 1010, 2634, 102]</td>\n",
              "      <td>[101, 102]</td>\n",
              "      <td>0</td>\n",
              "      <td>210</td>\n",
              "      <td>263</td>\n",
              "      <td>2</td>\n",
              "      <td>14689</td>\n",
              "      <td>0</td>\n",
              "      <td>1107</td>\n",
              "      <td>...</td>\n",
              "      <td>140</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>737</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 11924, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 3693, 1996, 3524, 9863, 100, 102]</td>\n",
              "      <td>[101, 1037, 2451, 1997, 9683, 1010, 2199, 1988...</td>\n",
              "      <td>0</td>\n",
              "      <td>85172</td>\n",
              "      <td>3</td>\n",
              "      <td>202</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>1096</td>\n",
              "      <td>...</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 4773, 10439, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 16938, 102]</td>\n",
              "      <td>[101, 102]</td>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>243</td>\n",
              "      <td>0</td>\n",
              "      <td>16659</td>\n",
              "      <td>1</td>\n",
              "      <td>4946</td>\n",
              "      <td>...</td>\n",
              "      <td>140</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1391</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 11924, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 102]</td>\n",
              "      <td>[101, 15536, 14416, 21541, 24532, 2102, 1064, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>291</td>\n",
              "      <td>1188</td>\n",
              "      <td>0</td>\n",
              "      <td>686</td>\n",
              "      <td>1</td>\n",
              "      <td>330</td>\n",
              "      <td>...</td>\n",
              "      <td>140</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 11924, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 2267, 2380, 1010, 9108, 102]</td>\n",
              "      <td>[101, 11268, 1999, 1996, 29466, 1997, 8827, 17...</td>\n",
              "      <td>0</td>\n",
              "      <td>896</td>\n",
              "      <td>837</td>\n",
              "      <td>1</td>\n",
              "      <td>3193</td>\n",
              "      <td>1</td>\n",
              "      <td>717</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 18059, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 102]</td>\n",
              "      <td>[101, 2023, 2003, 1996, 10474, 4070, 1997, 199...</td>\n",
              "      <td>0</td>\n",
              "      <td>5403</td>\n",
              "      <td>5464</td>\n",
              "      <td>57</td>\n",
              "      <td>21116</td>\n",
              "      <td>1</td>\n",
              "      <td>8101</td>\n",
              "      <td>...</td>\n",
              "      <td>140</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 2005, 18059, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1</td>\n",
              "      <td>[101, 2002, 1013, 2032, 102]</td>\n",
              "      <td>[101, 2280, 6951, 2005, 5325, 3793, 2240, 1012...</td>\n",
              "      <td>0</td>\n",
              "      <td>341</td>\n",
              "      <td>1171</td>\n",
              "      <td>1</td>\n",
              "      <td>2083</td>\n",
              "      <td>0</td>\n",
              "      <td>1854</td>\n",
              "      <td>...</td>\n",
              "      <td>144</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>10454</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10474, 4773, 10439, 102]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 2710, 102]</td>\n",
              "      <td>[101, 1056, 28394, 2102, 2055, 1001, 4105, 100...</td>\n",
              "      <td>0</td>\n",
              "      <td>7602</td>\n",
              "      <td>7231</td>\n",
              "      <td>226</td>\n",
              "      <td>11305</td>\n",
              "      <td>0</td>\n",
              "      <td>8788</td>\n",
              "      <td>...</td>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[101, 1056, 28394, 2102, 2214, 2695, 102]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 47 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20981972-6dcf-4848-9be8-abfcad0182ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20981972-6dcf-4848-9be8-abfcad0182ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20981972-6dcf-4848-9be8-abfcad0182ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating training and test dataset and using pbs batch function to get batched data set\n"
      ],
      "metadata": {
        "id": "W1RRg_D11r-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pds(example):\n",
        "  \"\"\"\n",
        "  Function used to isolate features and label for train/test\n",
        "  \"\"\"\n",
        "  label = (example[\"label\"] + 1) // 2\n",
        "  return {\n",
        "      \"user_location\": example[\"user_location\"],\n",
        "      \"user_description\": example[\"user_description\"],\n",
        "      \"protected\": example[\"protected\"],\n",
        "      \"followers_count\": example[\"followers_count\"],\n",
        "      \"friends_count\": example[\"friends_count\"],\n",
        "      \"listed_count\": example[\"listed_count\"],\n",
        "      \"favourites_count\": example[\"favourites_count\"],\n",
        "      \"geo_enabled\": example[\"geo_enabled\"],\n",
        "      \"statuses_count\": example[\"statuses_count\"],\n",
        "      \"contributors_enabled\": example[\"contributors_enabled\"],\n",
        "      \"is_translator\": example[\"is_translator\"],\n",
        "      \"is_translation_enabled\": example[\"is_translation_enabled\"],\n",
        "      \"profile_background_color\": example[\"profile_background_color\"],\n",
        "      \"profile_background_tile\": example[\"profile_background_tile\"],\n",
        "      \"profile_link_color\": example[\"profile_link_color\"],\n",
        "      \"profile_sidebar_border_color\": example[\"profile_sidebar_border_color\"],\n",
        "      \"profile_sidebar_fill_color\": example[\"profile_sidebar_fill_color\"],\n",
        "      \"profile_text_color\": example[\"profile_text_color\"],\n",
        "      \"profile_use_background_image\": example[\"profile_use_background_image\"],\n",
        "      \"translator_type\": example[\"translator_type\"],\n",
        "      \"has_extended_profile\": example[\"has_extended_profile\"],\n",
        "      \"default_profile\": example[\"default_profile\"],\n",
        "      \"default_profile_image\": example[\"default_profile_image\"],\n",
        "      \"tweet_frequency\": example[\"tweet_frequency\"],\n",
        "      \"followers_growth_rate\": example[\"followers_growth_rate\"],\n",
        "      \"friends_growth_rate\": example[\"friends_growth_rate\"],\n",
        "      \"favourites_growth_rate\": example[\"favourites_growth_rate\"],\n",
        "      \"listed_growth_rate\": example[\"listed_growth_rate\"],\n",
        "      \"followers_friends_ratio\": example[\"followers_friends_ratio\"],\n",
        "      \"screen_name_length\": example[\"screen_name_length\"],\n",
        "      \"num_digits_in_screen_name\": example[\"num_digits_in_screen_name\"],\n",
        "      \"name_length\": example[\"name_length\"],\n",
        "      \"num_digits_in_name\": example[\"num_digits_in_name\"],\n",
        "      \"description_length\": example[\"description_length\"],\n",
        "      \"media_types\": example[\"media_types\"],\n",
        "      \"tweet_text\": example[\"tweet_text\"],\n",
        "      \"length_of_text\": example[\"length_of_text\"],\n",
        "      \"is_retweet\": example[\"is_retweet\"],\n",
        "      \"is_reply\": example[\"is_reply\"],\n",
        "      \"number_of_tweet_mentions\": example[\"number_of_tweet_mentions\"],\n",
        "      \"quote_count\": example[\"quote_count\"],\n",
        "      \"retweet_count\": example[\"retweet_count\"],\n",
        "      \"is_retweeted\": example[\"is_retweeted\"],\n",
        "      \"favorite_count\": example[\"favorite_count\"],\n",
        "      \"contains_media\": example[\"contains_media\"],\n",
        "      \"source\": example[\"source\"],\n",
        "      }, label"
      ],
      "metadata": {
        "id": "HhikomYVRptc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_ds(df,\n",
        "                      embeddings_encoder_type = tf.int64):\n",
        "  \"\"\"\n",
        "  This function uses the pds function to map data from tabular format into the required tensorflow batched dataset.\n",
        "  The varaible embeddings_encoder_type controls the cast format of the embedded textual features. \n",
        "    It should reflect the embedded features and is defaulted to tf.int64\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  label = tf.convert_to_tensor(df[\"Bot\"], dtype = tf.int64)\n",
        "  #label = tf.ragged.constant(df[\"Bot\"], dtype = tf.int64)\n",
        "\n",
        "\n",
        "  user_location = tf.ragged.constant(df[\"user_location\"], dtype = embeddings_encoder_type)\n",
        "  user_description = tf.ragged.constant(df[\"user_description\"], dtype = embeddings_encoder_type)\n",
        "  protected = tf.ragged.constant(df[\"protected\"], dtype = tf.int64)\n",
        "  followers_count = tf.ragged.constant(df[\"followers_count\"], dtype = tf.int64)\n",
        "  friends_count = tf.ragged.constant(df[\"friends_count\"], dtype = tf.int64)\n",
        "  listed_count = tf.ragged.constant(df[\"listed_count\"], dtype = tf.int64)\n",
        "  favourites_count = tf.ragged.constant(df[\"favourites_count\"], dtype = tf.int64)\n",
        "  geo_enabled = tf.ragged.constant(df[\"geo_enabled\"], dtype = tf.int64)\n",
        "  statuses_count = tf.ragged.constant(df[\"statuses_count\"], dtype = tf.int64)\n",
        "  contributors_enabled = tf.ragged.constant(df[\"contributors_enabled\"], dtype = tf.int64)\n",
        "  is_translator = tf.ragged.constant(df[\"is_translator\"], dtype = tf.int64)\n",
        "  is_translation_enabled = tf.ragged.constant(df[\"is_translation_enabled\"], dtype = tf.int64)\n",
        "  profile_background_color = tf.ragged.constant(df[\"profile_background_color\"], dtype = embeddings_encoder_type)\n",
        "  profile_background_tile = tf.ragged.constant(df[\"profile_background_tile\"], dtype = tf.int64)\n",
        "  profile_link_color = tf.ragged.constant(df[\"profile_link_color\"], dtype = embeddings_encoder_type)\n",
        "  profile_sidebar_border_color = tf.ragged.constant(df[\"profile_sidebar_border_color\"], dtype = embeddings_encoder_type)\n",
        "  profile_sidebar_fill_color = tf.ragged.constant(df[\"profile_sidebar_fill_color\"], dtype = embeddings_encoder_type)\n",
        "  profile_text_color = tf.ragged.constant(df[\"profile_text_color\"], dtype = embeddings_encoder_type)\n",
        "  profile_use_background_image = tf.ragged.constant(df[\"profile_use_background_image\"], dtype = tf.int64)\n",
        "  translator_type = tf.ragged.constant(df[\"translator_type\"], dtype = embeddings_encoder_type)\n",
        "  has_extended_profile = tf.ragged.constant(df[\"has_extended_profile\"], dtype = tf.int64)\n",
        "  default_profile = tf.ragged.constant(df[\"default_profile\"], dtype = tf.int64)\n",
        "  default_profile_image = tf.ragged.constant(df[\"default_profile_image\"], dtype = tf.int64)\n",
        "  tweet_frequency = tf.ragged.constant(df[\"tweet_frequency\"], dtype = tf.int64)\n",
        "  followers_growth_rate = tf.ragged.constant(df[\"followers_growth_rate\"], dtype = tf.int64)\n",
        "  friends_growth_rate = tf.ragged.constant(df[\"friends_growth_rate\"], dtype = tf.int64)\n",
        "  favourites_growth_rate = tf.ragged.constant(df[\"favourites_growth_rate\"], dtype = tf.int64)\n",
        "  listed_growth_rate = tf.ragged.constant(df[\"listed_growth_rate\"], dtype = tf.int64)\n",
        "  followers_friends_ratio = tf.ragged.constant(df[\"followers_friends_ratio\"], dtype = tf.int64)\n",
        "  screen_name_length = tf.ragged.constant(df[\"screen_name_length\"], dtype = tf.int64)\n",
        "  num_digits_in_screen_name = tf.ragged.constant(df[\"num_digits_in_screen_name\"], dtype = tf.int64)\n",
        "  name_length = tf.ragged.constant(df[\"name_length\"], dtype = tf.int64)\n",
        "  num_digits_in_name = tf.ragged.constant(df[\"num_digits_in_name\"], dtype = tf.int64)\n",
        "  description_length = tf.ragged.constant(df[\"description_length\"], dtype = tf.int64)\n",
        "  media_types = tf.ragged.constant(df[\"media_types\"], dtype = embeddings_encoder_type)\n",
        "  tweet_text = tf.ragged.constant(df[\"tweet_text\"], dtype = embeddings_encoder_type)\n",
        "  length_of_text = tf.ragged.constant(df[\"length_of_text\"], dtype = tf.int64)\n",
        "  is_retweet = tf.ragged.constant(df[\"is_retweet\"], dtype = tf.int64)\n",
        "  is_reply = tf.ragged.constant(df[\"is_reply\"], dtype = tf.int64)\n",
        "  number_of_tweet_mentions = tf.ragged.constant(df[\"number_of_tweet_mentions\"], dtype = tf.int64)\n",
        "  quote_count = tf.ragged.constant(df[\"quote_count\"], dtype = tf.int64)\n",
        "  retweet_count = tf.ragged.constant(df[\"retweet_count\"], dtype = tf.int64)\n",
        "  is_retweeted = tf.ragged.constant(df[\"is_retweeted\"], dtype = tf.int64)\n",
        "  favorite_count = tf.ragged.constant(df[\"favorite_count\"], dtype = tf.int64)\n",
        "  contains_media = tf.ragged.constant(df[\"contains_media\"], dtype = tf.int64)\n",
        "  source = tf.ragged.constant(df[\"source\"], dtype = embeddings_encoder_type)\n",
        "\n",
        "\n",
        "  #train\n",
        "  return tf.data.Dataset.from_tensor_slices({'user_location': user_location,\n",
        "                                              'user_description': user_description,\n",
        "                                              'protected': protected,\n",
        "                                              'followers_count': followers_count,\n",
        "                                              'friends_count': friends_count,\n",
        "                                              'listed_count': listed_count,\n",
        "                                              'favourites_count': favourites_count,\n",
        "                                              'geo_enabled': geo_enabled,\n",
        "                                              'statuses_count': statuses_count,\n",
        "                                              'contributors_enabled': contributors_enabled,\n",
        "                                              'is_translator': is_translator,\n",
        "                                              'is_translation_enabled': is_translation_enabled,\n",
        "                                              'profile_background_color': profile_background_color,\n",
        "                                              'profile_background_tile': profile_background_tile,\n",
        "                                              'profile_link_color': profile_link_color,\n",
        "                                              'profile_sidebar_border_color': profile_sidebar_border_color,\n",
        "                                              'profile_sidebar_fill_color': profile_sidebar_fill_color,\n",
        "                                              'profile_text_color': profile_text_color,\n",
        "                                              'profile_use_background_image': profile_use_background_image,\n",
        "                                              'translator_type': translator_type,\n",
        "                                              'has_extended_profile': has_extended_profile,\n",
        "                                              'default_profile': default_profile,\n",
        "                                              'default_profile_image': default_profile_image,\n",
        "                                              'tweet_frequency': tweet_frequency,\n",
        "                                              'followers_growth_rate': followers_growth_rate,\n",
        "                                              'friends_growth_rate': friends_growth_rate,\n",
        "                                              'favourites_growth_rate': favourites_growth_rate,\n",
        "                                              'listed_growth_rate': listed_growth_rate,\n",
        "                                              'followers_friends_ratio': followers_friends_ratio,\n",
        "                                              'screen_name_length': screen_name_length,\n",
        "                                              'num_digits_in_screen_name': num_digits_in_screen_name,\n",
        "                                              'name_length': name_length,\n",
        "                                              'num_digits_in_name': num_digits_in_name,\n",
        "                                              'description_length': description_length,\n",
        "                                              'media_types': media_types,\n",
        "                                              'tweet_text': tweet_text,\n",
        "                                              'length_of_text': length_of_text,\n",
        "                                              'is_retweet': is_retweet,\n",
        "                                              'is_reply': is_reply,\n",
        "                                              'number_of_tweet_mentions': number_of_tweet_mentions,\n",
        "                                              'quote_count': quote_count,\n",
        "                                              'retweet_count': retweet_count,\n",
        "                                              'is_retweeted': is_retweeted,\n",
        "                                              'favorite_count': favorite_count,\n",
        "                                              'contains_media': contains_media,\n",
        "                                              'source': source,\n",
        "                                              'label': label\n",
        "                                              #'label': label\n",
        "                                              }).batch(1000).map(pds)\n",
        "\n",
        "\n",
        "train_ds = generate_batch_ds(df = df2,\n",
        "                             embeddings_encoder_type = tf.int64)\n",
        "\n",
        "test_ds = generate_batch_ds(df = df3,\n",
        "                             embeddings_encoder_type = tf.int64)"
      ],
      "metadata": {
        "id": "JWMcC3Dlz0Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "oruL_AkJSxRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model.\n",
        "m = Custom_keras_RandomForestModel(num_trees = 10,\n",
        "                                 verbose = 2,\n",
        "                                 max_depth = 20,\n",
        "                                 compute_oob_variable_importances = True,\n",
        "                                 compute_oob_performances = True,\n",
        "                                 random_seed = 42 #Same as used for train/test split\n",
        "                                 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model.\n",
        "m.fit(x=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzg_DRgOSmkR",
        "outputId": "e54a0870-6d9f-414c-c33c-4ca458b3af69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use 4 thread(s) for training\n",
            "Use /tmp/tmpnu_zdm1q as temporary training directory\n",
            "Reading training dataset...\n",
            "Training tensor examples:\n",
            "Features: {'user_location': tf.RaggedTensor(values=Tensor(\"data_55:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_56:0\", shape=(None,), dtype=int64)), 'user_description': tf.RaggedTensor(values=Tensor(\"data_53:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_54:0\", shape=(None,), dtype=int64)), 'protected': <tf.Tensor 'data_41:0' shape=(None,) dtype=int64>, 'followers_count': <tf.Tensor 'data_8:0' shape=(None,) dtype=int64>, 'friends_count': <tf.Tensor 'data_11:0' shape=(None,) dtype=int64>, 'listed_count': <tf.Tensor 'data_21:0' shape=(None,) dtype=int64>, 'favourites_count': <tf.Tensor 'data_6:0' shape=(None,) dtype=int64>, 'geo_enabled': <tf.Tensor 'data_13:0' shape=(None,) dtype=int64>, 'statuses_count': <tf.Tensor 'data_47:0' shape=(None,) dtype=int64>, 'contributors_enabled': <tf.Tensor 'data_1:0' shape=(None,) dtype=int64>, 'is_translator': <tf.Tensor 'data_19:0' shape=(None,) dtype=int64>, 'is_translation_enabled': <tf.Tensor 'data_18:0' shape=(None,) dtype=int64>, 'profile_background_color': tf.RaggedTensor(values=Tensor(\"data_29:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_30:0\", shape=(None,), dtype=int64)), 'profile_background_tile': <tf.Tensor 'data_31:0' shape=(None,) dtype=int64>, 'profile_link_color': tf.RaggedTensor(values=Tensor(\"data_32:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_33:0\", shape=(None,), dtype=int64)), 'profile_sidebar_border_color': tf.RaggedTensor(values=Tensor(\"data_34:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_35:0\", shape=(None,), dtype=int64)), 'profile_sidebar_fill_color': tf.RaggedTensor(values=Tensor(\"data_36:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_37:0\", shape=(None,), dtype=int64)), 'profile_text_color': tf.RaggedTensor(values=Tensor(\"data_38:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_39:0\", shape=(None,), dtype=int64)), 'profile_use_background_image': <tf.Tensor 'data_40:0' shape=(None,) dtype=int64>, 'translator_type': tf.RaggedTensor(values=Tensor(\"data_48:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_49:0\", shape=(None,), dtype=int64)), 'has_extended_profile': <tf.Tensor 'data_14:0' shape=(None,) dtype=int64>, 'default_profile': <tf.Tensor 'data_2:0' shape=(None,) dtype=int64>, 'default_profile_image': <tf.Tensor 'data_3:0' shape=(None,) dtype=int64>, 'tweet_frequency': <tf.Tensor 'data_50:0' shape=(None,) dtype=int64>, 'followers_growth_rate': <tf.Tensor 'data_10:0' shape=(None,) dtype=int64>, 'friends_growth_rate': <tf.Tensor 'data_12:0' shape=(None,) dtype=int64>, 'favourites_growth_rate': <tf.Tensor 'data_7:0' shape=(None,) dtype=int64>, 'listed_growth_rate': <tf.Tensor 'data_22:0' shape=(None,) dtype=int64>, 'followers_friends_ratio': <tf.Tensor 'data_9:0' shape=(None,) dtype=int64>, 'screen_name_length': <tf.Tensor 'data_44:0' shape=(None,) dtype=int64>, 'num_digits_in_screen_name': <tf.Tensor 'data_27:0' shape=(None,) dtype=int64>, 'name_length': <tf.Tensor 'data_25:0' shape=(None,) dtype=int64>, 'num_digits_in_name': <tf.Tensor 'data_26:0' shape=(None,) dtype=int64>, 'description_length': <tf.Tensor 'data_4:0' shape=(None,) dtype=int64>, 'media_types': tf.RaggedTensor(values=Tensor(\"data_23:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_24:0\", shape=(None,), dtype=int64)), 'tweet_text': tf.RaggedTensor(values=Tensor(\"data_51:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_52:0\", shape=(None,), dtype=int64)), 'length_of_text': <tf.Tensor 'data_20:0' shape=(None,) dtype=int64>, 'is_retweet': <tf.Tensor 'data_16:0' shape=(None,) dtype=int64>, 'is_reply': <tf.Tensor 'data_15:0' shape=(None,) dtype=int64>, 'number_of_tweet_mentions': <tf.Tensor 'data_28:0' shape=(None,) dtype=int64>, 'quote_count': <tf.Tensor 'data_42:0' shape=(None,) dtype=int64>, 'retweet_count': <tf.Tensor 'data_43:0' shape=(None,) dtype=int64>, 'is_retweeted': <tf.Tensor 'data_17:0' shape=(None,) dtype=int64>, 'favorite_count': <tf.Tensor 'data_5:0' shape=(None,) dtype=int64>, 'contains_media': <tf.Tensor 'data:0' shape=(None,) dtype=int64>, 'source': tf.RaggedTensor(values=Tensor(\"data_45:0\", shape=(None,), dtype=int64), row_splits=Tensor(\"data_46:0\", shape=(None,), dtype=int64))}\n",
            "Label: Tensor(\"data_57:0\", shape=(None,), dtype=int64)\n",
            "Weights: None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized tensor features:\n",
            " {'user_location': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_56:0\", shape=(None,), dtype=int64))), 'user_description': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_1:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_54:0\", shape=(None,), dtype=int64))), 'protected': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'followers_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'friends_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'listed_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_5:0' shape=(None,) dtype=float32>), 'favourites_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_6:0' shape=(None,) dtype=float32>), 'geo_enabled': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_7:0' shape=(None,) dtype=float32>), 'statuses_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_8:0' shape=(None,) dtype=float32>), 'contributors_enabled': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_9:0' shape=(None,) dtype=float32>), 'is_translator': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_10:0' shape=(None,) dtype=float32>), 'is_translation_enabled': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_11:0' shape=(None,) dtype=float32>), 'profile_background_color': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_2:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_30:0\", shape=(None,), dtype=int64))), 'profile_background_tile': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_13:0' shape=(None,) dtype=float32>), 'profile_link_color': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_3:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_33:0\", shape=(None,), dtype=int64))), 'profile_sidebar_border_color': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_4:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_35:0\", shape=(None,), dtype=int64))), 'profile_sidebar_fill_color': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_5:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_37:0\", shape=(None,), dtype=int64))), 'profile_text_color': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_6:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_39:0\", shape=(None,), dtype=int64))), 'profile_use_background_image': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_18:0' shape=(None,) dtype=float32>), 'translator_type': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_7:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_49:0\", shape=(None,), dtype=int64))), 'has_extended_profile': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_20:0' shape=(None,) dtype=float32>), 'default_profile': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_21:0' shape=(None,) dtype=float32>), 'default_profile_image': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_22:0' shape=(None,) dtype=float32>), 'tweet_frequency': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_23:0' shape=(None,) dtype=float32>), 'followers_growth_rate': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_24:0' shape=(None,) dtype=float32>), 'friends_growth_rate': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_25:0' shape=(None,) dtype=float32>), 'favourites_growth_rate': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_26:0' shape=(None,) dtype=float32>), 'listed_growth_rate': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_27:0' shape=(None,) dtype=float32>), 'followers_friends_ratio': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_28:0' shape=(None,) dtype=float32>), 'screen_name_length': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_29:0' shape=(None,) dtype=float32>), 'num_digits_in_screen_name': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_30:0' shape=(None,) dtype=float32>), 'name_length': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_31:0' shape=(None,) dtype=float32>), 'num_digits_in_name': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_32:0' shape=(None,) dtype=float32>), 'description_length': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_33:0' shape=(None,) dtype=float32>), 'media_types': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_8:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_24:0\", shape=(None,), dtype=int64))), 'tweet_text': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_9:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_52:0\", shape=(None,), dtype=int64))), 'length_of_text': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_36:0' shape=(None,) dtype=float32>), 'is_retweet': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_37:0' shape=(None,) dtype=float32>), 'is_reply': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_38:0' shape=(None,) dtype=float32>), 'number_of_tweet_mentions': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_39:0' shape=(None,) dtype=float32>), 'quote_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_40:0' shape=(None,) dtype=float32>), 'retweet_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_41:0' shape=(None,) dtype=float32>), 'is_retweeted': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_42:0' shape=(None,) dtype=float32>), 'favorite_count': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_43:0' shape=(None,) dtype=float32>), 'contains_media': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_44:0' shape=(None,) dtype=float32>), 'source': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"Add_10:0\", shape=(None,), dtype=int32), row_splits=Tensor(\"data_46:0\", shape=(None,), dtype=int64)))}\n",
            "Training dataset read in 0:00:05.585306. Found 1000 examples.\n",
            "Training model...\n",
            "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training gets stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO 2023-05-11T06:42:00.52523958+00:00 kernel.cc:756] Start Yggdrasil model training\n",
            "[INFO 2023-05-11T06:42:00.525339692+00:00 kernel.cc:757] Collect training examples\n",
            "[INFO 2023-05-11T06:42:00.525557288+00:00 kernel.cc:388] Number of batches: 1\n",
            "[INFO 2023-05-11T06:42:00.525566483+00:00 kernel.cc:389] Number of examples: 1000\n",
            "[INFO 2023-05-11T06:42:00.532145933+00:00 kernel.cc:774] Training dataset:\n",
            "Number of records: 1000\n",
            "Number of columns: 47\n",
            "\n",
            "Number of columns by type:\n",
            "\tNUMERICAL: 35 (74.4681%)\n",
            "\tCATEGORICAL_SET: 11 (23.4043%)\n",
            "\tCATEGORICAL: 1 (2.12766%)\n",
            "\n",
            "Columns:\n",
            "\n",
            "NUMERICAL: 35 (74.4681%)\n",
            "\t1: \"contains_media\" NUMERICAL mean:0.212 min:0 max:1 sd:0.408725\n",
            "\t2: \"contributors_enabled\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t3: \"default_profile\" NUMERICAL mean:0.393 min:0 max:1 sd:0.488417\n",
            "\t4: \"default_profile_image\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t5: \"description_length\" NUMERICAL mean:114.777 min:0 max:190 sd:44.6882\n",
            "\t6: \"favorite_count\" NUMERICAL mean:343.782 min:0 max:35687 sd:2339.07\n",
            "\t7: \"favourites_count\" NUMERICAL mean:29913.9 min:0 max:1.16998e+06 sd:73599.5\n",
            "\t8: \"favourites_growth_rate\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t9: \"followers_count\" NUMERICAL mean:1.01031e+06 min:2 max:1.33497e+08 sd:7.40302e+06\n",
            "\t10: \"followers_friends_ratio\" NUMERICAL mean:-1.84467e+16 min:-9.22337e+18 max:2.98827e+06 sd:4.12069e+17\n",
            "\t11: \"followers_growth_rate\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t12: \"friends_count\" NUMERICAL mean:7209.83 min:0 max:1.4365e+06 sd:56204.2\n",
            "\t13: \"friends_growth_rate\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t14: \"geo_enabled\" NUMERICAL mean:0.56 min:0 max:1 sd:0.496387\n",
            "\t15: \"has_extended_profile\" NUMERICAL mean:0.449 min:0 max:1 sd:0.497392\n",
            "\t16: \"is_reply\" NUMERICAL mean:1 min:1 max:1 sd:0\n",
            "\t17: \"is_retweet\" NUMERICAL mean:0.367 min:0 max:1 sd:0.481987\n",
            "\t18: \"is_retweeted\" NUMERICAL mean:0.657 min:0 max:1 sd:0.474711\n",
            "\t19: \"is_translation_enabled\" NUMERICAL mean:0.017 min:0 max:1 sd:0.129271\n",
            "\t20: \"is_translator\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t21: \"length_of_text\" NUMERICAL mean:134.861 min:10 max:319 sd:74.2181\n",
            "\t22: \"listed_count\" NUMERICAL mean:2552.99 min:0 max:218427 sd:12568.2\n",
            "\t23: \"listed_growth_rate\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t25: \"name_length\" NUMERICAL mean:15.321 min:1 max:50 sd:8.08034\n",
            "\t26: \"num_digits_in_name\" NUMERICAL mean:0.06 min:0 max:8 sd:0.492341\n",
            "\t27: \"num_digits_in_screen_name\" NUMERICAL mean:0.26 min:0 max:8 sd:0.958332\n",
            "\t28: \"number_of_tweet_mentions\" NUMERICAL mean:1.155 min:0 max:17 sd:1.4377\n",
            "\t30: \"profile_background_tile\" NUMERICAL mean:0.157 min:0 max:1 sd:0.363801\n",
            "\t35: \"profile_use_background_image\" NUMERICAL mean:0.741 min:0 max:1 sd:0.438086\n",
            "\t36: \"protected\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t37: \"quote_count\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\t38: \"retweet_count\" NUMERICAL mean:710.369 min:0 max:122285 sd:6197.56\n",
            "\t39: \"screen_name_length\" NUMERICAL mean:10.902 min:3 max:15 sd:2.73649\n",
            "\t41: \"statuses_count\" NUMERICAL mean:39135 min:30 max:1.51746e+06 sd:100162\n",
            "\t43: \"tweet_frequency\" NUMERICAL mean:0 min:0 max:0 sd:0\n",
            "\n",
            "CATEGORICAL_SET: 11 (23.4043%)\n",
            "\t24: \"media_types\" CATEGORICAL_SET integerized vocab-size:104 no-ood-item\n",
            "\t29: \"profile_background_color\" CATEGORICAL_SET integerized vocab-size:29564 no-ood-item\n",
            "\t31: \"profile_link_color\" CATEGORICAL_SET integerized vocab-size:29539 no-ood-item\n",
            "\t32: \"profile_sidebar_border_color\" CATEGORICAL_SET integerized vocab-size:29294 no-ood-item\n",
            "\t33: \"profile_sidebar_fill_color\" CATEGORICAL_SET integerized vocab-size:29493 no-ood-item\n",
            "\t34: \"profile_text_color\" CATEGORICAL_SET integerized vocab-size:29329 no-ood-item\n",
            "\t40: \"source\" CATEGORICAL_SET integerized vocab-size:28396 no-ood-item\n",
            "\t42: \"translator_type\" CATEGORICAL_SET integerized vocab-size:3906 no-ood-item\n",
            "\t44: \"tweet_text\" CATEGORICAL_SET integerized vocab-size:30267 no-ood-item\n",
            "\t45: \"user_description\" CATEGORICAL_SET integerized vocab-size:30267 no-ood-item\n",
            "\t46: \"user_location\" CATEGORICAL_SET integerized vocab-size:29843 no-ood-item\n",
            "\n",
            "CATEGORICAL: 1 (2.12766%)\n",
            "\t0: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n",
            "\n",
            "Terminology:\n",
            "\tnas: Number of non-available (i.e. missing) values.\n",
            "\tood: Out of dictionary.\n",
            "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
            "\ttokenized: The attribute value is obtained through tokenization.\n",
            "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
            "\tvocab-size: Number of unique values.\n",
            "\n",
            "[INFO 2023-05-11T06:42:00.532882986+00:00 kernel.cc:790] Configure learner\n",
            "[INFO 2023-05-11T06:42:00.533733803+00:00 kernel.cc:804] Training config:\n",
            "learner: \"RANDOM_FOREST\"\n",
            "features: \"^contains_media$\"\n",
            "features: \"^contributors_enabled$\"\n",
            "features: \"^default_profile$\"\n",
            "features: \"^default_profile_image$\"\n",
            "features: \"^description_length$\"\n",
            "features: \"^favorite_count$\"\n",
            "features: \"^favourites_count$\"\n",
            "features: \"^favourites_growth_rate$\"\n",
            "features: \"^followers_count$\"\n",
            "features: \"^followers_friends_ratio$\"\n",
            "features: \"^followers_growth_rate$\"\n",
            "features: \"^friends_count$\"\n",
            "features: \"^friends_growth_rate$\"\n",
            "features: \"^geo_enabled$\"\n",
            "features: \"^has_extended_profile$\"\n",
            "features: \"^is_reply$\"\n",
            "features: \"^is_retweet$\"\n",
            "features: \"^is_retweeted$\"\n",
            "features: \"^is_translation_enabled$\"\n",
            "features: \"^is_translator$\"\n",
            "features: \"^length_of_text$\"\n",
            "features: \"^listed_count$\"\n",
            "features: \"^listed_growth_rate$\"\n",
            "features: \"^media_types$\"\n",
            "features: \"^name_length$\"\n",
            "features: \"^num_digits_in_name$\"\n",
            "features: \"^num_digits_in_screen_name$\"\n",
            "features: \"^number_of_tweet_mentions$\"\n",
            "features: \"^profile_background_color$\"\n",
            "features: \"^profile_background_tile$\"\n",
            "features: \"^profile_link_color$\"\n",
            "features: \"^profile_sidebar_border_color$\"\n",
            "features: \"^profile_sidebar_fill_color$\"\n",
            "features: \"^profile_text_color$\"\n",
            "features: \"^profile_use_background_image$\"\n",
            "features: \"^protected$\"\n",
            "features: \"^quote_count$\"\n",
            "features: \"^retweet_count$\"\n",
            "features: \"^screen_name_length$\"\n",
            "features: \"^source$\"\n",
            "features: \"^statuses_count$\"\n",
            "features: \"^translator_type$\"\n",
            "features: \"^tweet_frequency$\"\n",
            "features: \"^tweet_text$\"\n",
            "features: \"^user_description$\"\n",
            "features: \"^user_location$\"\n",
            "label: \"^__LABEL$\"\n",
            "task: CLASSIFICATION\n",
            "random_seed: 42\n",
            "metadata {\n",
            "  framework: \"TF Keras\"\n",
            "}\n",
            "pure_serving_model: false\n",
            "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
            "  num_trees: 10\n",
            "  decision_tree {\n",
            "    max_depth: 20\n",
            "    min_examples: 5\n",
            "    in_split_min_examples_check: true\n",
            "    keep_non_leaf_label_distribution: true\n",
            "    num_candidate_attributes: 0\n",
            "    missing_value_policy: GLOBAL_IMPUTATION\n",
            "    allow_na_conditions: false\n",
            "    categorical_set_greedy_forward {\n",
            "      sampling: 0.1\n",
            "      max_num_items: -1\n",
            "      min_item_frequency: 1\n",
            "    }\n",
            "    growing_strategy_local {\n",
            "    }\n",
            "    categorical {\n",
            "      cart {\n",
            "      }\n",
            "    }\n",
            "    axis_aligned_split {\n",
            "    }\n",
            "    internal {\n",
            "      sorting_strategy: PRESORTED\n",
            "    }\n",
            "    uplift {\n",
            "      min_examples_in_treatment: 5\n",
            "      split_score: KULLBACK_LEIBLER\n",
            "    }\n",
            "  }\n",
            "  winner_take_all_inference: true\n",
            "  compute_oob_performances: true\n",
            "  compute_oob_variable_importances: true\n",
            "  num_oob_variable_importances_permutations: 1\n",
            "  bootstrap_training_dataset: true\n",
            "  bootstrap_size_ratio: 1\n",
            "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
            "  sampling_with_replacement: true\n",
            "}\n",
            "\n",
            "[INFO 2023-05-11T06:42:00.534404595+00:00 kernel.cc:807] Deployment config:\n",
            "cache_path: \"/tmp/tmpnu_zdm1q/working_cache\"\n",
            "num_threads: 4\n",
            "try_resume_training: true\n",
            "\n",
            "[INFO 2023-05-11T06:42:00.534683676+00:00 kernel.cc:868] Train model\n",
            "[INFO 2023-05-11T06:42:00.535331281+00:00 random_forest.cc:415] Training random forest on 1000 example(s) and 46 feature(s).\n",
            "[INFO 2023-05-11T06:42:00.745512963+00:00 random_forest.cc:804] Training of tree  1/10 (tree index:2) done accuracy:0.892388 logloss:3.87871\n",
            "[INFO 2023-05-11T06:42:01.328113501+00:00 random_forest.cc:804] Training of tree  10/10 (tree index:9) done accuracy:0.933333 logloss:2.16913\n",
            "[INFO 2023-05-11T06:42:01.338914447+00:00 random_forest.cc:884] Final OOB metrics: accuracy:0.933333 logloss:2.16913\n",
            "[INFO 2023-05-11T06:42:01.339777764+00:00 feature_importance.cc:192] Running 47 features on 4 threads with 1 rounds\n",
            "[INFO 2023-05-11T06:42:01.353556908+00:00 kernel.cc:905] Export model in log directory: /tmp/tmpnu_zdm1q with prefix 696f0cff52614b5d\n",
            "[INFO 2023-05-11T06:42:01.355248244+00:00 kernel.cc:923] Save model in resources\n",
            "[INFO 2023-05-11T06:42:01.361151201+00:00 abstract_model.cc:849] Model self evaluation:\n",
            "Number of predictions (without weights): 990\n",
            "Number of predictions (with weights): 990\n",
            "Task: CLASSIFICATION\n",
            "Label: __LABEL\n",
            "\n",
            "Accuracy: 0.933333  CI95[W][0.918801 0.945906]\n",
            "LogLoss: : 2.16913\n",
            "ErrorRate: : 0.0666667\n",
            "\n",
            "Default Accuracy: : 0.938384\n",
            "Default LogLoss: : 0.231391\n",
            "Default ErrorRate: : 0.0616162\n",
            "\n",
            "Confusion Table:\n",
            "truth\\prediction\n",
            "   0    1  2\n",
            "0  0    0  0\n",
            "1  0  923  6\n",
            "2  0   60  1\n",
            "Total: 990\n",
            "\n",
            "One vs other classes:\n",
            "\n",
            "[INFO 2023-05-11T06:42:01.37368408+00:00 kernel.cc:1214] Loading model from path /tmp/tmpnu_zdm1q/model/ with prefix 696f0cff52614b5d\n",
            "[INFO 2023-05-11T06:42:01.391103872+00:00 decision_forest.cc:661] Model loaded with 10 root(s), 484 node(s), and 30 input feature(s).\n",
            "[INFO 2023-05-11T06:42:01.391148749+00:00 abstract_model.cc:1311] Engine \"RandomForestGeneric\" built\n",
            "[INFO 2023-05-11T06:42:01.39117903+00:00 kernel.cc:1046] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained in 0:00:00.884629\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fe251f27ac0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7fe251f27ac0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe24d6815d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}
